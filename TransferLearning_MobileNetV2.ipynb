{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202fd01a-3eb5-4a92-80c2-b6d01e8c2e21",
   "metadata": {},
   "source": [
    "## Transfer Learning with MobileNetV2\n",
    "\n",
    "MobileNetV2, was designed to provide fast and computationally efficient performance. It's been pre-trained on ImageNet, a dataset containing over 14 million images and 1000 classes.\n",
    "\n",
    "At the end of this notebook, you will be able to:\n",
    "\n",
    "- Create a dataset from a directory\n",
    "- Preprocess and augment data using the Sequential API\n",
    "- Adapt a pretrained model to new data and train a classifier using the Functional API and MobileNet\n",
    "- Fine-tune a classifier's final layers to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa3843-0afa-4d8e-8e70-c78808894baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory #Just set the training set to subset='training' and the validation set to subset='validation' in this function\n",
    "from keras.layers import RandomFlip, RandomRotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073f01d-0f0c-4203-9b20-f4ca7d37908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)\n",
    "\n",
    "directory = \"dataset/\"\n",
    "train_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='training',\n",
    "                                             seed=42)\n",
    "validation_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='validation',\n",
    "                                             seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c524be1-8105-43a4-804f-3e32952a9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's take a look at some of the images from the training set:\n",
    "class_names = train_dataset.class_names\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606da80-f064-4605-9248-d77f4039bb2a",
   "metadata": {},
   "source": [
    "## Preprocess and Augment Training Data \n",
    "\n",
    "Using prefetch() prevents a memory bottleneck that can occur when reading from disk. It sets aside some data and keeps it ready for when it's needed, by creating a source dataset from your input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. Because the iteration is streaming ( chunks at a time = little fixed memory), the data doesn't need to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1400f03c-0e9a-424d-a656-f9a27478fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE  #Optimizes buffer size\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb465b71-ab11-4194-baa3-3612357ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmenter():\n",
    "    '''\n",
    "    Create a Sequential model composed of 2 layers\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    '''\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential()\n",
    "    data_augmentation.add(RandomFlip('horizontal'))\n",
    "    data_augmentation.add(RandomRotation(0.2))\n",
    "    \n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3fece-4d2c-4a0f-9d65-d5d51f61e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = data_augmenter()\n",
    "\n",
    "for image, _ in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = image[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f2b8e-6f9a-40dd-9c6a-d1da12091ded",
   "metadata": {},
   "source": [
    "What you should remember:\n",
    "\n",
    "- When calling image_data_set_from_directory(), specify the train/val subsets and match the seeds to prevent overlap\n",
    "- Use prefetch() to prevent memory bottlenecks when reading from disk\n",
    "- Give your model more to learn from with simple data augmentations like rotation and flipping.\n",
    "- When using a pretrained model, it's best to reuse the weights it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78505e9-a805-488c-a79e-fb7bbc5baf14",
   "metadata": {},
   "source": [
    "## Using MobileNetV2 for Transfer Learning\n",
    "\n",
    "MobileNetV2 was trained on ImageNet and is optimized to run on mobile and other low-power applications. It's 155 layers deep.\n",
    "\n",
    "The architecture has three defining characteristics:\n",
    "\n",
    "- Depthwise separable convolutions\n",
    "- Thin input and output bottlenecks between layers\n",
    "- Shortcut connections between bottleneck layers\n",
    "\n",
    "MobileNetV2 uses depthwise separable convolutions as efficient building blocks. Traditional convolutions are often very resource-intensive, and  depthwise separable convolutions are able to reduce the number of trainable parameters and operations and also speed up convolutions in two steps: \n",
    "\n",
    "1. The first step calculates an intermediate result by convolving on each of the channels independently. This is the depthwise convolution.\n",
    "\n",
    "2. In the second step, another convolution merges the outputs of the previous step into one. This gets a single result from a single feature at a time, and then is applied to all the filters in the output layer. This is the pointwise convolution, or: **Shape of the depthwise convolution X Number of filters.**\n",
    "\n",
    "<img src=\"images/mobilenetv2.png\" style=\"width:650px;height:450px;\">\n",
    "\n",
    "Each block consists of an inverted residual structure with a bottleneck at each end. These bottlenecks encode the intermediate inputs and outputs in a low dimensional space, and prevent non-linearities from destroying important information. \n",
    "\n",
    "The shortcut connections, which are similar to the ones in traditional residual networks, serve the same purpose of speeding up training and improving predictions. These connections skip over the intermediate convolutions and connect the bottleneck layers. \n",
    "\n",
    "<font color='blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "\n",
    "* MobileNetV2's unique features are: \n",
    "  * Depthwise separable convolutions that provide lightweight feature filtering and creation\n",
    "  * Input and output bottlenecks that preserve important information on either end of the block\n",
    "* Depthwise separable convolutions deal with both spatial and depth (number of channels) dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e378d6-63a7-486c-87db-a7ec09228a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to train your base model using all the layers from the pretrained model.\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model_path=\"imagenet_base_model/with_top_mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160.h5\"\n",
    "\n",
    "#By specifying weights='imagenet', the weights are downloaded for ImageNet. But for this notebook, we'll load them locally from this workspace.\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=True,\n",
    "                                               weights=base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384bcc6-ea74-4e4e-8ff7-bd63207d7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c940b4e-d854-45dd-a480-255100e76bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note the last 2 layers here. They are the so called top layers, and they are responsible of the classification in the model\n",
    "\n",
    "nb_layers = len(base_model.layers)\n",
    "print(base_model.layers[nb_layers - 2].name)\n",
    "print(base_model.layers[nb_layers - 1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886bf8f-5994-4201-96b7-58c136862942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets test out the  MobileNetV2 base model\n",
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5a154-f496-41b3-a2df-f6a8ad7d2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number 32 above refers to the batch size and 1000 above refers to the 1000 classes the model was pretrained on. \n",
    "\n",
    "#Shows the different label probabilities in one tensor \n",
    "label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fbd3d-9321-4784-991e-ccb954fa7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "image_var = tf.Variable(preprocess_input(image_batch))\n",
    "pred = base_model(image_var)\n",
    "\n",
    "# Function to decode predictions\n",
    "def decode_predictions(preds, top=2):\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        result = [tuple(class_index[str(i)]) + (pred[i],) for i in top_indices]\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "with open(\"imagenet_base_model/imagenet_class_index.json\", 'r') as f:\n",
    "    class_index = json.load(f)\n",
    "    \n",
    "decoded_predictions = decode_predictions(pred.numpy(), top=2)\n",
    "print(decoded_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c611ac-9df2-427d-8dd9-387806b01461",
   "metadata": {},
   "source": [
    "The predictions returned by the base model below follow this format: First the class number, then a human-readable label, and last the probability of the image belonging to that class. You'll notice that there are two of these returned for each image in the batch - these the top two probabilities returned for that image.\n",
    "\n",
    "\n",
    "There's a whole lot of labels here, some of them hilariously wrong, but none of them say \"alpaca.\" This is because MobileNet pretrained over ImageNet doesn't have the correct labels for alpacas, so when you use the full model, all you get is a bunch of incorrectly classified images.\n",
    "\n",
    "Fortunately, you can delete the top layer, which contains all the classification labels, and create a new classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9a491-17c0-42d3-867d-d25c4b7e46e5",
   "metadata": {},
   "source": [
    "## Layer Freezing with the Functional API\n",
    "\n",
    "You'll see how you can use a pretrained model to modify the classifier task so that it's able to recognize alpacas. You can achieve this in three steps: \n",
    "\n",
    "1. Delete the top layer (the classification layer)\n",
    "    * Set `include_top` in `base_model` as False\n",
    "2. Add a new classifier layer\n",
    "    * Train only one layer by freezing the rest of the network\n",
    "    * As mentioned before, a single neuron is enough to solve a binary classification problem.\n",
    "3. Freeze the base model and train the newly-created classifier layer\n",
    "    * Set `base model.trainable=False` to avoid changing the weights and train *only* the new layer\n",
    "    * Set training in `base_model` to False to avoid keeping track of statistics in the batch norm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314843a-8154-46dd-b077-7efc3a320de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
    "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
    "    Arguments:\n",
    "        image_shape -- Image width and height\n",
    "        data_augmentation -- data augmentation function\n",
    "    Returns:\n",
    "        tf.keras.model\n",
    "    '''\n",
    "   \n",
    "    input_shape = image_shape + (3,) #image_shape = (60, 60), then input_shape = image_shape + (3,) evaluates to (60, 60, 3)\n",
    "      \n",
    "    base_model_path = \"imagenet_base_model/without_top_mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5\"\n",
    "   \n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,           # \u2190 use the actual shape here\n",
    "        include_top=False,                 # \u2190 Important: remove classification head\n",
    "        weights=base_model_path            # local weights file (no_top version)\n",
    "    )\n",
    "   \n",
    "    # Freeze the base model by making it non-trainable\n",
    "    base_model.trainable = False\n",
    "   \n",
    "    # Create the input layer (using the correct image shape)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "   \n",
    "    # Apply data augmentation to the inputs\n",
    "    x = data_augmentation(inputs)\n",
    "   \n",
    "    # Data preprocessing using the same function MobileNetV2 was trained with\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "   \n",
    "    # Pass through the frozen base model\n",
    "    # Set training=False to freeze batch norm statistics (important when base_model is frozen)\n",
    "    x = base_model(x, training=False)\n",
    "   \n",
    "    # Add the new binary classification layers\n",
    "    # Use global average pooling to summarize spatial info into one value per channel\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "   \n",
    "    # Use a prediction layer with one neuron \n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "   \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448b9a8-c9cb-4a95-b542-674b51c3b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = alpaca_model(IMG_SIZE, data_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47a463-fe09-423a-8ecb-3e140ce2caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04fb63-f9bb-41d3-aabe-e1704ddb1271",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 5\n",
    "history = model2.fit(train_dataset, validation_data=validation_dataset, epochs=initial_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db09dc-5e1c-4832-8d12-445fa34f1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training and validation accuracy:\n",
    "\n",
    "acc = [0.] + history.history['accuracy']\n",
    "val_acc = [0.] + history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad116b-5cfd-4d7d-bc3c-4c37ce8d195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results are ok, but could be better. Next, try some fine-tuning.\n",
    "\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa5065-7f56-4ca5-b66b-76a0627476c5",
   "metadata": {},
   "source": [
    "## Fine-tuning the Model\n",
    "\n",
    "In transfer learning, the way you achieve this is by unfreezing the layers at the end of the network, and then re-training your model on the final layers with a very low learning rate. Adapting your learning rate to go over these layers in smaller steps can yield more fine details - and higher accuracy.To achieve this, just unfreeze the final layers and re-run the optimizer with a smaller learning rate, while keeping all the other layers frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04fd44-6717-4ec1-8849-e6f3ecf9cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model2.layers):\n",
    "    print(f\"Index {i}: {layer.__class__.__name__}  -  name: {layer.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa158f07-6c73-48f0-844e-8bcade4981e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model2.layers[2]\n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74860b3-e025-4b72-9fda-6881f4562b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c22d0-02a9-4b3f-9534-7808e2b90408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Fine tune at Layer 120 onwards\n",
    "fine_tune_at = 120\n",
    "\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False     # \u2190 use False (not None)\n",
    "\n",
    "# Define a BinaryCrossentropy loss function. Use from_logits=True\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.1 * base_learning_rate)\n",
    "\n",
    "# Use accuracy as evaluation metric\n",
    "metrics = ['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7a4d1-4c11-486b-aa17-3ab1a93b487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=loss_function,\n",
    "              optimizer = optimizer,\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de13d64-8946-47ea-8579-388efa150d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model2.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)\n",
    "\n",
    "\"\"\" \n",
    "history is the object returned from the first.fit() call.history.epoch is a list of the epoch numbers that were actually run during that first training phase.\n",
    "#initial_epochs = 5, then history.epoch == [0, 1, 2, 3, 4], So history.epoch[-1] == 4 (the last epoch that was completed)\n",
    "Keras begins counting epochs from initial_epoch (not from 0 again).Result \u2192 the epochs in the fine-tuning phase are numbered 5, 6, 7, 8, 9\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd3eb8-166b-4faa-b40a-09b3b6ed034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quite an improvement! A little fine-tuning can really go a long way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ee1e1-6ada-489d-9919-d68668f0ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd1321-98ff-4e37-a6ef-ec591dc87710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e3a1f-7e37-4d01-ae27-ad893575b504",
   "metadata": {},
   "source": [
    "**What you should remember**:\n",
    "\n",
    "* To adapt the classifier to new data: Delete the top layer, add a new classification layer, and train only on that layer\n",
    "* When freezing layers, avoid keeping track of statistics (like in the batch normalization layer)\n",
    "* Fine-tune the final layers of your model to capture high-level details near the end of the network and potentially improve accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7b295-5426-496c-bde3-6107b59cb851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}