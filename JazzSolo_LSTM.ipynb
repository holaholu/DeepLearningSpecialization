{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcd81d8-fb2b-4d8b-8a43-b3e99fa8f8aa",
   "metadata": {},
   "source": [
    "# Improvise a Jazz Solo with an LSTM Network\n",
    "\n",
    " In this notebook, we will implement a model that uses an LSTM to generate music. At the end, you'll even be able to listen to your own music! \n",
    "\n",
    "<img src=\"images/jazz.jpg\" style=\"width:450;height:300px;\">\n",
    "\n",
    "**By the end of this assignment, you'll be able to:**\n",
    "\n",
    "\n",
    "- Apply an LSTM to a music generation task\n",
    "- Generate your own jazz music with deep learning\n",
    "- Use the flexible Functional API to create complex models\n",
    "\n",
    "This is going to be a fun one. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a78485-636d-49cd-bf52-dd046cb0aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff45b5-97d8-4855-befa-ea000820df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install audioop-lts\n",
    "import IPython\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import *\n",
    "from jazzlstm.grammar import *\n",
    "from jazzlstm.qa import *\n",
    "from jazzlstm.preprocess import * \n",
    "from jazzlstm.music_utils import *\n",
    "from jazzlstm.data_utils import *\n",
    "from jazzlstm.outputs import *\n",
    "from jazzlstm.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe351bd-f21a-47a8-88fb-1d7bd780c947",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Problem Statement\n",
    "\n",
    "You would like to create a jazz music piece specially for a friend's birthday. However, you don't know how to play any instruments, or how to compose music. Fortunately, you know deep learning and will solve this problem using an LSTM network! \n",
    "\n",
    "You will train a network to generate novel jazz solos in a style representative of a body of performed work. \ud83d\ude0e\ud83c\udfb7\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Dataset\n",
    "\n",
    "To get started, you'll train your algorithm on a corpus of Jazz music. Run the cell below to listen to a snippet of the audio from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb439c08-ca82-4447-bdc1-f3369a8389e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('./data/30s_seq.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8f9ba-5f31-4111-9e12-ca889f8cdd89",
   "metadata": {},
   "source": [
    "#### What are musical \"values\"? (optional)\n",
    "You can informally think of each \"value\" as a note, which comprises a pitch and duration. For example, if you press down a specific piano key for 0.5 seconds, then you have just played a note. In music theory, a \"value\" is actually more complicated than this -- specifically, it also captures the information needed to play multiple notes at the same time. For example, when playing a music piece, you might press down two piano keys at the same time (playing multiple notes at the same time generates what's called a \"chord\"). But you don't need to worry about the details of music theory for this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4023a-124a-4955-920f-dcfb07d0c8cf",
   "metadata": {},
   "source": [
    "The preprocessing of the musical data has been taken care of already, which for this notebook means it's been rendered in terms of musical \"values.\" \n",
    "* For the purposes of this notebook, all you need to know is that you'll obtain a dataset of values, and will use an RNN model to generate sequences of values. \n",
    "* Your music generation system will use 90 unique values. \n",
    "\n",
    "Run the following code to load the raw music data and preprocess it into values. This might take a few minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d1e7d-ad0b-48dd-adc2-1d7ac7857fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, n_values, indices_values, chords = load_music_utils('data/original_metheny.mid')\n",
    "print('number of training examples:', X.shape[0])\n",
    "print('Tx (length of sequence):', X.shape[1])\n",
    "print('total # of unique values:', n_values)\n",
    "print('shape of X:', X.shape)\n",
    "print('Shape of Y:', Y.shape)\n",
    "print('Number of chords', len(chords))\n",
    "\n",
    "\n",
    "print(\"Data loaded. n_values =\", n_values)\n",
    "print(\"Number of measures:\", len(measures) if 'measures' in globals() else \"measures not returned \u2014 see below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526e741-2b07-4507-9077-6e94661e1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaper = Reshape((1, n_values))                  # uses real n_values (57)\n",
    "LSTM_cell = LSTM(n_a, return_state=True)\n",
    "densor = Dense(n_values, activation='softmax')     # output layer now has 57 units\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc8f25-ed65-4210-95bb-948627c50ef6",
   "metadata": {},
   "source": [
    "You have just loaded the following:\n",
    "\n",
    "- `X`: This is an (m, $T_x$, 90) dimensional array. \n",
    "    - You have m training examples, each of which is a snippet of $T_x =30$ musical values. \n",
    "    - At each time step, the input is one of 90 different possible values, represented as a one-hot vector. \n",
    "        - For example, X[i,t,:] is a one-hot vector representing the value of the i-th example at time t. \n",
    "\n",
    "- `Y`: a $(T_y, m, 90)$ dimensional array\n",
    "    - This is essentially the same as `X`, but shifted one step to the left (to the past). \n",
    "    - Notice that the data in `Y` is **reordered** to be dimension $(T_y, m, 90)$, where $T_y = T_x$. This format makes it more convenient to feed into the LSTM later.\n",
    "    - Similar to the dinosaur assignment, you're using the previous values to predict the next value.\n",
    "        - So your sequence model will try to predict $y^{\\langle t \\rangle}$ given $x^{\\langle 1\\rangle}, \\ldots, x^{\\langle t \\rangle}$. \n",
    "\n",
    "- `n_values`: The number of unique values in this dataset. This should be 90. \n",
    "\n",
    "- `indices_values`: python dictionary mapping integers 0 through 89 to musical values.\n",
    "\n",
    "- `chords`: Chords used in the input midi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bff1c3-19ff-4b6f-b9ca-d2fa53b86296",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Model Overview\n",
    "\n",
    "Here is the architecture of the model you'll use.you'll implement it in Keras.\n",
    "\n",
    "<img src=\"images/music_generation.png\" style=\"width:600;height:400px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: Basic LSTM model </center></caption>\n",
    "\n",
    "\n",
    "* $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\cdots, x^{\\langle T_x \\rangle})$ is a window of size $T_x$ scanned over the musical corpus. \n",
    "* Each $x^{\\langle t \\rangle}$ is an index corresponding to a value.\n",
    "* $\\hat{y}^{\\langle t \\rangle}$ is the prediction for the next value.\n",
    "* You'll be training the model on random snippets of 30 values taken from a much longer piece of music. \n",
    "    - Thus, you won't bother to set the first input $x^{\\langle 1 \\rangle} = \\vec{0}$, since most of these snippets of audio start somewhere in the middle of a piece of music. \n",
    "    - You're setting each of the snippets to have the same length $T_x = 30$ to make vectorization easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b20b9a-dacf-4888-815f-11f7f1d8ab87",
   "metadata": {},
   "source": [
    "## 2 - Building the Model\n",
    "\n",
    "Now, you'll build and train a model that will learn musical patterns. \n",
    "* The model takes input X of shape $(m, T_x, 90)$ and labels Y of shape $(T_y, m, 90)$. \n",
    "* You'll use an LSTM with hidden states that have $n_{a} = 64$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a732f4f-2397-4acb-b3c0-6fe6d3a42bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions for the hidden state of each LSTM cell.\n",
    "n_a = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047abeb-5ee0-4175-953b-4463d9bd6705",
   "metadata": {},
   "source": [
    "#### Sequence generation uses a for-loop\n",
    "* If you're building an RNN where, at test time, the entire input sequence $x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\ldots, x^{\\langle T_x \\rangle}$ is given in advance, then Keras has simple built-in functions to build the model. \n",
    "* However, for **sequence generation, at test time you won't know all the values of $x^{\\langle t\\rangle}$ in advance**.\n",
    "* Instead, you'll generate them one at a time using $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$. \n",
    "    * The input at time \"t\" is the prediction at the previous time step \"t-1\".\n",
    "* So you'll need to implement your own for-loop to iterate over the time steps. \n",
    "\n",
    "#### Shareable weights\n",
    "* The function `djmodel()` will call the LSTM layer $T_x$ times using a for-loop.\n",
    "* It is important that all $T_x$ copies have the same weights. \n",
    "    - The $T_x$ steps should have shared weights that aren't re-initialized.\n",
    "* Referencing a globally defined shared layer will utilize the same layer-object instance at each time step.\n",
    "* The key steps for implementing layers with shareable weights in Keras are: \n",
    "1. Define the layer objects (you'll use global variables for this).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "#### 3 types of layers\n",
    "* The layer objects you need for global variables have been defined.  \n",
    "    * Just run the next cell to create them! \n",
    "* Please read the Keras documentation and understand these layers: \n",
    "    - [Reshape()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape): Reshapes an output to a certain shape.\n",
    "    - [LSTM()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Long Short-Term Memory layer\n",
    "    - [Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): A regular fully-connected neural network layer.\n",
    "\n",
    "\n",
    "* `reshaper`, `LSTM_cell` and `densor` are globally defined layer objects that you'll use to implement `djmodel()`. \n",
    "* In order to propagate a Keras tensor object X through one of these layers, use `layer_object()`.\n",
    "    - For one input, use `layer_object(X)`\n",
    "    - For more than one input, put the inputs in a list: `layer_object([X1,X2])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4488bd-8f40-4449-b194-fd40b1025216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def djmodel(Tx, LSTM_cell, densor, reshaper):\n",
    "    \"\"\"\n",
    "    Implement the djmodel composed of Tx LSTM cells where each cell is responsible\n",
    "    for learning the following note based on the previous note and context.\n",
    "    \"\"\"\n",
    "    # Get shapes\n",
    "    n_values = densor.units\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define inputs\n",
    "    X = Input(shape=(Tx, n_values), name='X')\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    \n",
    "    # Initialize states\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for t in range(Tx):\n",
    "        # Select t-th time step\n",
    "        x = X[:, t, :]\n",
    "        \n",
    "        # Reshape to (batch, 1, n_values)\n",
    "        x = reshaper(x)\n",
    "        \n",
    "        # LSTM step \u2013 IMPORTANT: pass x positionally, NOT with inputs=\n",
    "        _, a, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        \n",
    "        # Dense prediction\n",
    "        out = densor(a)\n",
    "        \n",
    "        # Collect output\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[X, a0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cc918-0308-4531-ad58-5637a4f0cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOU CANNOT EDIT THIS CELL\n",
    "\n",
    "\n",
    "\n",
    "model = djmodel(Tx=30, LSTM_cell=LSTM_cell, densor=densor, reshaper=reshaper)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[['accuracy']] * 30    # one accuracy metric per output\n",
    ")\n",
    "\n",
    "print(\"Model built with correct n_values =\", n_values)\n",
    "print(\"Expected X shape:\", model.inputs[0].shape)   # should be (None, 30, 57)\n",
    "print(\"Actual X shape:\", X.shape)                   # should match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8151f4-7842-4052-9522-7e5eb5576939",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5d9f2-a038-41b1-91b2-c5fa4e25d313",
   "metadata": {},
   "source": [
    "#### Initialize hidden state and cell state\n",
    "Finally, let's initialize `a0` and `c0` for the LSTM's initial state to be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae382d-92c5-4e2a-9de8-febe8b36fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 60\n",
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbf8b2-46d7-46fe-b37a-faf760f546d8",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "You're now ready to fit the model! \n",
    "\n",
    "* You'll turn `Y` into a list, since the cost function expects `Y` to be provided in this format. \n",
    "    - `list(Y)` is a list with 30 items, where each of the list items is of shape (60,90). \n",
    "    - Train for 100 epochs (This will take a few minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb981c3a-78bb-487c-9ed5-b34fc631cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X, a0, c0], y = list(Y), epochs=100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d06be4-bbef-4515-afe8-7a95bf6b3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"loss at epoch 1: {history.history['loss'][0]}\")\n",
    "print(f\"loss at epoch 100: {history.history['loss'][99]}\")\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4aab2-1c86-4b50-9cf4-47ca98f2ac51",
   "metadata": {},
   "source": [
    "## 3 - Generating Music\n",
    "\n",
    "You now have a trained model which has learned the patterns of a jazz soloist. You can now use this model to synthesize new music! \n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Predicting & Sampling\n",
    "\n",
    "<img src=\"images/music_gen.png\" style=\"width:600;height:400px;\">\n",
    "<center><caption><b><font color='purple'>Figure 2: Generating new values in an LSTM </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d40208-47bb-403a-a139-113cedef628e",
   "metadata": {},
   "source": [
    "At each step of sampling, you will:\n",
    "* Take as input the activation '`a`' and cell state '`c`' from the previous state of the LSTM.\n",
    "* Forward propagate by one step.\n",
    "* Get a new output activation, as well as cell state. \n",
    "* The new activation '`a`' can then be used to generate the output using the fully connected layer, `densor`. \n",
    "\n",
    "#### Initialization\n",
    "* You'll initialize the following to be zeros:\n",
    "    * `x0` \n",
    "    * hidden state `a0` \n",
    "    * cell state `c0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cf3fc-5a46-4068-85c8-8495efd706f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def music_inference_model(LSTM_cell, densor, Ty=50):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \"\"\"\n",
    "    n_values = densor.units\n",
    "    n_a = LSTM_cell.units\n",
    "\n",
    "    # Input for first timestep (start token / zero vector)\n",
    "    x0 = Input(shape=(1, n_values), name='x0')\n",
    "\n",
    "    # Initial states\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for t in range(Ty):\n",
    "        # LSTM step\n",
    "        _, a, c = LSTM_cell(x, initial_state=[a, c])\n",
    "\n",
    "        # Dense \u2192 probabilities\n",
    "        out = densor(a)\n",
    "\n",
    "        # Append current timestep prediction (useful for debugging / sampling)\n",
    "        outputs.append(out)\n",
    "\n",
    "        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        # Next input: argmax \u2192 one-hot \u2192 (None, 1, n_values)\n",
    "        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "        # 1. argmax \u2192 index of highest probability (shape: (None,))\n",
    "        pred_index = Lambda(\n",
    "            lambda y: tf.math.argmax(y, axis=-1),\n",
    "            output_shape=()                    # scalar per batch item\n",
    "        )(out)\n",
    "\n",
    "        # 2. one-hot encode the predicted index\n",
    "        # Output shape: (None, n_values)\n",
    "        x_onehot = Lambda(\n",
    "            lambda idx: tf.one_hot(idx, depth=n_values),\n",
    "            output_shape=(n_values,)           # \u2190 required: explicit shape\n",
    "        )(pred_index)\n",
    "\n",
    "        # 3. RepeatVector(1) \u2192 add time dimension\n",
    "        x = RepeatVector(1)(x_onehot)\n",
    "\n",
    "    # Create the inference model\n",
    "    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)\n",
    "\n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867295a-541c-4e3c-a695-c4d5c0fb7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the cell below to define your inference model. This model is hard coded to generate 50 values.\n",
    "inference_model = music_inference_model(LSTM_cell, densor, Ty = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061b80f-a993-49fc-ae7b-a5f60d146995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the inference model\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5829344-ae5b-4d19-853a-c48268a973bc",
   "metadata": {},
   "source": [
    "#### Initialize inference model\n",
    "The following code creates the zero-valued vectors you will use to initialize `x` and the LSTM state variables `a` and `c`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5590e86-f077-41a4-be62-60c87813a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, n_values))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325fb74-c629-4f4e-877a-26177f6b890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer,\n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "   \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, 90), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cell\n",
    "   \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, n_values), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "   \n",
    "    n_values = x_initializer.shape[2]\n",
    "   \n",
    "    \n",
    "    # Step 1: Use your inference model to predict an output sequence \n",
    "    # given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])\n",
    "    \n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "    \n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(indices, num_classes=n_values)\n",
    "    \n",
    "   \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e4876-8caf-423f-bd6c-5da06e098860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n",
    "\n",
    "print(\"np.argmax(results[12]) =\", np.argmax(results[12]))\n",
    "print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "print(\"list(indices[12:18]) =\", list(indices[12:18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df13eb1-1388-4075-a5fd-f73ffae7ecdb",
   "metadata": {},
   "source": [
    "### 3.2 - Generate Music \n",
    "\n",
    "Finally! You're ready to generate music. \n",
    "\n",
    "Your RNN generates a sequence of values. The following code generates music by first calling your `predict_and_sample()` function. These values are then post-processed into musical chords (meaning that multiple values or notes can be played at the same time). \n",
    "\n",
    "Most computational music algorithms use some post-processing because it's difficult to generate music that sounds good without it. The post-processing does things like clean up the generated audio by making sure the same sound is not repeated too many times, or that two successive notes are not too far from each other in pitch, and so on. \n",
    "\n",
    "One could argue that a lot of these post-processing steps are hacks; also, a lot of the music generation literature has also focused on hand-crafting post-processors, and a lot of the output quality depends on the quality of the post-processing and not just the quality of the model. But this post-processing does make a huge difference, so you should use it in your implementation as well. \n",
    "\n",
    "Let's make some music! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72c1a3-bb40-4e20-a86a-9c4d4d1ac301",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_stream = generate_music(inference_model, indices_values, chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fc6bc-8094-442a-b3c7-2a320f98c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid2wav('output/my_music.midi')\n",
    "IPython.display.Audio('./output/rendered.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e14e9-d77f-4b70-a656-2cd3b4cd56f8",
   "metadata": {},
   "source": [
    "To listen to your music, click File->Open... Then go to \"output/\" and download \"my_music.midi\". Either play it on your computer with an application that can read midi files if you have one, or use one of the free online \"MIDI to mp3\" conversion tools to convert this to mp3.  \n",
    "\n",
    "As a reference, here is a 30 second audio clip generated using this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4d51f-a4ba-49c4-b940-306a3fd0b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('./data/30s_trained_model.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b5c8d-8999-487c-bf1c-094ee8d41262",
   "metadata": {},
   "source": [
    "<font color='blue'><b> What you should remember:</b>\n",
    "    \n",
    "- A sequence model can be used to generate musical values, which are then post-processed into midi music. \n",
    "- You can use a fairly similar model for tasks ranging from generating dinosaur names to generating original music, with the only major difference being the input fed to the model.  \n",
    "- In Keras, sequence generation involves defining layers with shared weights, which are then repeated for the different time steps $1, \\ldots, T_x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea7834-3726-44a6-9a9d-de69ee3cbd75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLspe (Python 3.13)",
   "language": "python",
   "name": "dlspe_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}