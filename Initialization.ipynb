{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bef1d1c-0854-47c8-9a29-41bd8d886eb0",
   "metadata": {},
   "source": [
    "A well-chosen initialization can:\n",
    "\n",
    "- Speed up the convergence of gradient descent.\n",
    "- Increase the odds of gradient descent converging to a lower training (and generalization) error.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7be10d-5bc0-4384-9d21-1cfa319c79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98714466-b3f1-4983-a58e-02d392bfb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image dataset: blue/red dots in circles\n",
    "def load_dataset():\n",
    "    np.random.seed(1)\n",
    "    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)\n",
    "    np.random.seed(2)\n",
    "    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)\n",
    "    # Visualize the data\n",
    "    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "    test_X = test_X.T\n",
    "    test_Y = test_Y.reshape((1, test_Y.shape[0]))\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "    \n",
    "train_X, train_Y, test_X, test_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b7fe81-98e5-48b6-bd01-f97afa8e98d9",
   "metadata": {},
   "source": [
    "These are the initialization methods we'll experiment with:\n",
    "\n",
    "Zeros initialization -- setting initialization = \"zeros\" in the input argument.\n",
    "Random initialization -- setting initialization = \"random\" in the input argument. This initializes the weights to large random values.\n",
    "He initialization -- setting initialization = \"he\" in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415f824-2b32-4273-b5ef-d5c5418f0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let`s first import all helper functions\n",
    "def sigmoid(x):\n",
    "   \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "        \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "   \n",
    "    m = X.shape[1]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = 1./m * (a3 - Y)\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "   \n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for k in range(L):\n",
    "        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
    "        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def compute_loss(a3, Y):\n",
    "    \n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "    loss = 1./m * np.nansum(logprobs)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def load_cat_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "    \n",
    "    train_set_x = train_set_x_orig/255\n",
    "    test_set_x = test_set_x_orig/255\n",
    "\n",
    "    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n",
    "\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "  \n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print results\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    \n",
    "def predict_dec(parameters, X):\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (a3>0.5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6339265-b73e-4e76-88ce-ea3c9fad530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lets define the model\n",
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_iterations -- number of iterations to run gradient descent\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the loss\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "\n",
    "    # Initialize parameters dictionary.\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "\n",
    "        # Loss\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the loss\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64984b3e-2516-4a3f-9992-76bf9f80038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize with Paramaters with zero\n",
    "\n",
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab0b3f-32d7-40e8-a436-b9e929639b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_zeros([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bcb73-25a1-4bc3-8a73-0d355dc65d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets train the model to test it out\n",
    "parameters = model(train_X, train_Y, initialization = \"zeros\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a5e45-c79d-4d0a-aeb6-6b1073c20d2a",
   "metadata": {},
   "source": [
    "The performance is terrible, the cost doesn't decrease, and the algorithm performs no better than random guessing. \n",
    "Why? Take a look at the details of the predictions and the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068342a-84a8-4fc4-b0c1-44d0ccdd51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"predictions_train = \" + str(predictions_train))\n",
    "print (\"predictions_test = \" + str(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec3aaa-af8c-47ce-b9a9-3f645c240503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with Zeros initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1480e-ac59-4a8d-a9cd-f8d4a1dc035f",
   "metadata": {},
   "source": [
    "Since the weights and biases are zero, multiplying by the weights creates the zero vector which gives 0 when the activation function is ReLU. As z = 0\n",
    "As for every example you are getting a 0.5 chance of it being true our cost function becomes helpless in adjusting the weights.This is why you can see that the model is predicting 0 for every example! No wonder it's doing so badly.\n",
    "\n",
    "In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so you might as well be training a neural network with  \ud835\udc5b[\ud835\udc59]=1\n",
    "for every layer. This way, the network is no more powerful than a linear classifier like logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3937c9b-a48a-43ef-bc64-0b1e68fce52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Initialization\n",
    "def initialize_parameters_random(layers_dims):\n",
    "\n",
    "    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as mine\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # integer representing the number of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6415c-8866-4e0f-adc8-43b2bd6dc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_random([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58dbe89-c1aa-4fb6-a70b-a8a3d3b0fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets train the randomly initialized model:\n",
    "parameters = model(train_X, train_Y, initialization = \"random\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e9cb1-f913-4519-b509-59b06ada7438",
   "metadata": {},
   "source": [
    "This gives noticeably better accuracy than before. The model is no longer outputting all 0s. Progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007b1cb-5258-4c3c-950d-5bd6000d3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predictions_train)\n",
    "print (predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b33057-090a-4de9-acc9-5b7850800e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with large random initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3184051-4afd-43e5-820c-87ad437cb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try the He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d348a3f-0491-44eb-ab41-d912e0fdd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c47fc-f1c7-4edc-baa6-86ebddfaed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887425f3-705e-44a2-a24d-ee8a9abfa134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let train the model again\n",
    "parameters = model(train_X, train_Y, initialization = \"he\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237985dd-03fe-4a85-8c22-ae1a30b3cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d28266-672c-4405-a74a-232bae5323f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "The model with He initialization separates the blue and the red dots very well in a small number of iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}