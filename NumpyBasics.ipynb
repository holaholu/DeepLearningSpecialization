{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69c38d0-8193-4654-ba80-7bbf144aa464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57596279-2020-4d70-88f4-3b3f28f70ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_sigmoid(x):\n",
    "    #Compute sigmoid of x\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86294fd4-e598-4d7d-9b7b-7d6e33390f80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basic_sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\u001b[39;00m\n\u001b[32m      3\u001b[39m x = [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m] \u001b[38;5;66;03m# x becomes a python list object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mbasic_sigmoid\u001b[49m(x) \u001b[38;5;66;03m# you will see this give an error when you run it, because x is a vector.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'basic_sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "\n",
    "x = [1, 2, 3] # x becomes a python list object\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832087ae-03c6-4856-909e-2f86e1cda7d3",
   "metadata": {},
   "source": [
    "In fact, if  ùë•=(ùë•1,ùë•2,...,ùë•ùëõ) is a row vector then np.exp(x) will apply the exponential function to every element of x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad09a5b-6e99-4564-82f6-bfc35877afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example of np.exp\n",
    "t_x = np.array([1, 2, 3])\n",
    "print(np.exp(t_x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4e41bd-6fc7-44f6-80f2-c94d9eca714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# example of vector operation\n",
    "t_x = np.array([1, 2, 3])\n",
    "print (t_x + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a40807b1-32c0-4d02-a309-ecdd13653f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(t_x) = [0.73105858 0.88079708 0.95257413]\n"
     ]
    }
   ],
   "source": [
    "#using numpy instead\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24219376-1a35-4929-af31-4383b3789951",
   "metadata": {},
   "source": [
    "Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is:\n",
    "\n",
    "ùë†ùëñùëîùëöùëúùëñùëë_ùëëùëíùëüùëñùë£ùëéùë°ùëñùë£ùëí(ùë•)=ùúé‚Ä≤(ùë•)=ùúé(ùë•)(1‚àíùúé(ùë•))(2)\n",
    "\n",
    "You often code this function in two steps:\n",
    "\n",
    "Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "Compute  ùúé‚Ä≤(ùë•)=ùë†(1‚àíùë†)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "615d6069-75a0-43fa-80a2-9d7b878160fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(t_x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ds = s * (1 - s)\n",
    "    return ds\n",
    "t_x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43a00e2c-7433-4a6a-8636-eba3a8000064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using reshape function\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "\n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    v = image.reshape(-1, 1)\n",
    "    \n",
    "    #You can also explicitly multiply the dimensions, though it is more verbose:\n",
    "    #v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2]), 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "653dfeb0-8bef-445d-8168-9f500b358373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(t_image)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f31f3-a9f7-42b0-91e0-2cb9af39d30a",
   "metadata": {},
   "source": [
    "Normalize_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b4c15ff-d69e-4ceb-a01d-40110d6a5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "\n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "\n",
    "    # Divide x by its norm.\n",
    "    x = x / x_norm\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a933f28-9529-4a0d-b85e-3fc94b03854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 3., 4.],\n",
    "              [1., 6., 4.]])\n",
    "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbad01bb-0f34-4cbb-b74c-a1e600321cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "#Softmax\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "\n",
    "    #(‚âà 3 lines of code)\n",
    "\n",
    "\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return s\n",
    "t_x = np.array([[9, 2, 5, 0, 0],\n",
    "                [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(t_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe205035-4b3a-417f-a1a3-a613ff09af32",
   "metadata": {},
   "source": [
    "VECTORIZATION\n",
    "\n",
    "In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a\n",
    "huge bottleneck in your algorithm and can result in a model that takes ages to run.\n",
    "\n",
    "Lets see the difference between the following implementations of the dot/outer/elementwise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76b6b1ff-13cf-43db-915c-b39d536fd354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.10699999999985721ms\n",
      "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.7220000000001114ms\n",
      "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- Computation time = 0.15900000000002024ms\n",
      "gdot = [28.98629054 20.05343666 33.16438861]\n",
      " ----- Computation time = 0.1829999999998222ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1), len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j] * x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14b52463-3e02-4529-b565-e0363828622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.25000000000030553ms\n",
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.2260000000000595ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- Computation time = 0.220000000000109ms\n",
      "gdot = [28.98629054 20.05343666 33.16438861]\n",
      " ----- Computation time = 0.29199999999995896ms\n"
     ]
    }
   ],
   "source": [
    "#now lets try Vectorization\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4357ed0-2dfa-4220-8a7b-cf547a018355",
   "metadata": {},
   "source": [
    "Note that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to .* in Matlab/Octave), which performs an element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20149db3-4b4c-4888-b1f3-1687faf8d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "\"\"\" L1 loss function\n",
    "L1 loss is defined as:\n",
    "ùêø1(ùë¶ÃÇ ,ùë¶)=‚àëùëñ=0ùëö‚àí1|ùë¶(ùëñ)‚àíùë¶ÃÇ (ùëñ)|\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "\n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "\n",
    "    loss = np.sum(np.abs(y - yhat))\n",
    "\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13a320b4-7f18-419f-a09c-28f49f9fed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "L2 loss is defined as\n",
    "ùêø2(ùë¶ÃÇ ,ùë¶)=‚àëùëñ=0ùëö‚àí1(ùë¶(ùëñ)‚àíùë¶ÃÇ (ùëñ))^2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "\n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    loss = np.sum((y - yhat)**2)\n",
    "    return loss\n",
    "    \n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L2 = \" + str(L2(yhat, y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea996d-c31a-4ee1-af28-41db85b24779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
