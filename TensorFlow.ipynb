{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76753d07-d9ff-4b5b-853e-ee9f6747b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from tensorflow.python.ops.resource_variable_ops import ResourceVariable\n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5390ad7-c8bf-4817-9433-fe4288fe6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e312a94-2918-4d99-976e-3b5b9d3768f7",
   "metadata": {},
   "source": [
    "The beauty of TensorFlow 2 is in its simplicity. Basically, all you need to do is implement forward propagation through a computational graph. TensorFlow will compute the derivatives for you, by moving backwards through the graph recorded with GradientTape. All that's left for you to do then is specify the cost function and optimizer you want to use!\n",
    "\n",
    "When writing a TensorFlow program, the main object to get used and transformed is the tf.Tensor. These tensors are the TensorFlow equivalent of Numpy arrays. you'll use tf.Variable to store the state of your variables.the dtype arg in tf.Variable can be set to allow data to be converted to that type.\n",
    "\n",
    "Here you'll call the TensorFlow dataset created on a HDF5 file, which you can use in place of a Numpy array to store your datasets. You can think of this as a TensorFlow data generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca631c-18a8-4c12-931e-71bbd704ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "test_dataset = h5py.File('datasets/test_signs.h5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c52f8e-2c26-4d72-a404-b698bf25c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_x'])\n",
    "y_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_y'])\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_x'])\n",
    "y_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeeeb84-94f9-47eb-b7ef-8994137370ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf215d-0b7e-48f9-aafd-b74becc97d97",
   "metadata": {},
   "source": [
    "Since TensorFlow Datasets are generators, you can't access directly the contents unless you iterate over them in a for loop, or by explicitly creating a Python iterator using iter and consuming its elements using next. Also, you can inspect the shape and dtype of each element using the element_spec attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6bcec-e364-4fdf-bd46-50c87d826d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de0858-c3be-4b1e-859a-239212f0611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(x_train)))  # Each Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb1ff7-79d5-4b03-996f-15541ec20542",
   "metadata": {},
   "source": [
    "The dataset that you'll be using is a subset of the sign language digits. It contains six different classes representing the digits from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f594f-2f9e-4f15-99f8-a4318009e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for element in y_train:\n",
    "    unique_labels.add(element.numpy())\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8600d9-532e-4ff0-b7d8-a89582b0ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_iter = iter(x_train)\n",
    "labels_iter = iter(y_train)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(next(images_iter).numpy().astype(\"uint8\"))\n",
    "    plt.title(next(labels_iter).numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2ceb2-3ba1-4abd-9b8d-a13a0f7cc258",
   "metadata": {},
   "source": [
    "There's one more additional difference between TensorFlow datasets and Numpy arrays: If you need to transform one, you would invoke the map method to apply the function passed as an argument to each of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb4183-371d-4941-a7b1-55ce622da68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    \"\"\"\n",
    "    Transform an image into a tensor of shape (64 * 64 * 3, )\n",
    "    and normalize its components.\n",
    "\n",
    "    Arguments\n",
    "    image - Tensor.\n",
    "\n",
    "    Returns:\n",
    "    result -- Transformed tensor\n",
    "    \"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [-1,])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab81bb9-e633-4a39-93cd-c746ff5213b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = x_train.map(normalize)\n",
    "new_test = x_test.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d02a5-93c4-4728-b13e-351227a51d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4800f2f-9576-44e9-8c01-3b513096ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(new_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1447d6-ff67-403f-9463-d1a104dfb9cc",
   "metadata": {},
   "source": [
    "## Linear Function\n",
    "\n",
    "you can modify the state of a tf.Variable but cannot change the state of a tf.constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7e7a3-b9a2-49eb-8994-da263339e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(np.random.randn(3,1), name = \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570c7b0-dec2-4e40-9882-23486b7e201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function:\n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns:\n",
    "    result -- Y = WX + b\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    \"\"\"\n",
    "    Note, to ensure that the \"random\" numbers generated match the expected results,\n",
    "    please create the variables in the order given in the starting code below.\n",
    "    (Do not re-arrange the order).\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.random.randn(3, 1)\n",
    "    W = np.random.randn(4, 3)\n",
    "    b = np.random.randn(4, 1)\n",
    "    Y = tf.matmul(W, X) + b\n",
    "\n",
    "    return Y\n",
    "\n",
    "result = linear_function()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b6994-b01e-4fb8-881a-804256e1d89f",
   "metadata": {},
   "source": [
    "## Computing the Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e67e9-6644-4bba-b00a-9c6b8e933320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "\n",
    "    Returns:\n",
    "    a -- (tf.float32) the sigmoid of z\n",
    "    \"\"\"\n",
    "    # tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.\n",
    "\n",
    "    z = tf.cast(z, tf.float32)\n",
    "    a = tf.keras.activations.sigmoid(z)   \n",
    "    return a\n",
    "\n",
    "\n",
    "result = sigmoid(-1)\n",
    "print (\"type: \" + str(type(result)))\n",
    "print (\"dtype: \" + str(result.dtype))\n",
    "print (\"sigmoid(-1) = \" + str(result))\n",
    "print (\"sigmoid(0) = \" + str(sigmoid(0.0)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40273f1c-9297-4e82-8647-c088f59c53b4",
   "metadata": {},
   "source": [
    "## Using One Hot Encodings\n",
    "\n",
    "In \"one hot\" encoding, exactly one element of each column is \"hot\" (meaning set to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d97110-c034-4ed1-af8c-7f3c75e84c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, C=6):\n",
    "    \"\"\"\n",
    "\u00a0\u00a0\u00a0\u00a0Computes the one hot encoding for a single label\n",
    "\u00a0\u00a0\u00a0\u00a0\n",
    "\u00a0\u00a0\u00a0\u00a0Arguments:\n",
    "        label --  (int) Categorical labels\n",
    "        C --  (int) Number of different classes that label can take\n",
    "\u00a0\u00a0\u00a0\u00a0\n",
    "\u00a0\u00a0\u00a0\u00a0Returns:\n",
    "         one_hot -- tf.Tensor A one-dimensional tensor (array) with the one hot encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot = tf.one_hot(indices=label, depth=C)\n",
    "    one_hot = tf.reshape(one_hot, [C])\n",
    "    return one_hot\n",
    "    \n",
    "label = tf.constant(1)\n",
    "\n",
    "C = 6\n",
    "one_hot_matrix(label, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf88394-d4ea-4ec5-a950-dbcaf045ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_y_test = y_test.map(one_hot_matrix)\n",
    "new_y_train = y_train.map(one_hot_matrix)\n",
    "print(next(iter(new_y_test))) #prints one labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032fe1a-2245-4d61-bf03-805ba320ae4c",
   "metadata": {},
   "source": [
    "## Initialize the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90641a-f538-4317-87df-b791c986e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with TensorFlow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=1)\n",
    "\n",
    "    W1 = tf.Variable(initializer(shape=(25, 12288)), name=\"W1\")\n",
    "    b1 = tf.Variable(initializer(shape=(25, 1)),     name=\"b1\")\n",
    "    W2 = tf.Variable(initializer(shape=(12, 25)),    name=\"W2\")\n",
    "    b2 = tf.Variable(initializer(shape=(12, 1)),     name=\"b2\")\n",
    "    W3 = tf.Variable(initializer(shape=(6, 12)),     name=\"W3\")\n",
    "    b3 = tf.Variable(initializer(shape=(6, 1)),      name=\"b3\")\n",
    "\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496868b-6048-4da6-8498-3f305a59c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters ()\n",
    "for key in parameters:\n",
    "        print(f\"{key} shape: {tuple(parameters[key].shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5636756-b0c8-442b-8cc5-235adf73f1ff",
   "metadata": {},
   "source": [
    "## Building Your First Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ddd65-c0b8-4e98-ade2-26b0e8131281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    #(approx. 5 lines)                   # Numpy Equivalents (NumPy not to be used. Use TF API):\n",
    "    # Z1 = ...                           # Z1 = np.dot(W1, X) + b1\n",
    "    # A1 = ...                           # A1 = relu(Z1)\n",
    "    # Z2 = ...                           # Z2 = np.dot(W2, A1) + b2\n",
    "    # A2 = ...                           # A2 = relu(Z2)\n",
    "    # Z3 = ...                           # Z3 = np.dot(W3, A2) + b3\n",
    "\n",
    "    Z1 = tf.linalg.matmul(W1, X) + b1\n",
    "    A1 = tf.keras.activations.relu(Z1)\n",
    "    Z2 = tf.linalg.matmul(W2, A1) + b2\n",
    "    A2 = tf.keras.activations.relu(Z2)\n",
    "    Z3 = tf.linalg.matmul(W3, A2) + b3\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35cb32-df7c-4cbe-b39c-587abd65dba3",
   "metadata": {},
   "source": [
    "## Compute the Total Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baff02c-33e8-4445-b74e-c3ae34c89c70",
   "metadata": {},
   "source": [
    "All you have to do now is define the loss function that you're going to use. For this case, since we have a classification problem with 6 labels, a categorical cross entropy will work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa636c4c-5071-4a77-b9f5-95e010ca87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the total loss\n",
    "\n",
    "    Arguments:\n",
    "    logits -- output of forward propagation (output of the last LINEAR unit), of shape (6, num_examples)\n",
    "    labels -- \"true\" labels vector, same shape as Z3\n",
    "\n",
    "    Returns:\n",
    "    total_loss - Tensor of the total loss value\n",
    "    \"\"\"\n",
    "\n",
    "    # compute total loss. remember to set `from_logits=True`\n",
    "    total_loss = tf.reduce_sum(\n",
    "        tf.keras.losses.categorical_crossentropy( tf.transpose(labels),  tf.transpose(logits), from_logits=True) \n",
    "    )\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "#Lets test it\n",
    "labels = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n",
    "logits = tf.constant([[1., 0., 0.], [1., 0., 0.], [1., 0., 0.]])\n",
    "\n",
    "result = compute_total_loss(logits, labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954ae61-4e42-4cbd-8d26-b03db2ce7a57",
   "metadata": {},
   "source": [
    "Note: When using sum of losses for gradient computation, it\u2019s important to reduce the learning rate as the size of the mini-batch increases. This ensures that you don\u2019t take large steps towards minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff42825-e5ad-4716-ac74-9e2ba5d759c9",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "tape.gradient function: this allows you to retrieve the operations recorded for automatic differentiation inside the GradientTape block. Then, calling the optimizer method apply_gradients, will apply the optimizer's update rules to each trainable parameter.\n",
    "\n",
    "tf.Data.dataset = dataset.prefetch(8) : prevents a memory bottleneck that can occur when reading from disk. prefetch() sets aside some data and keeps it ready for when it's needed. It does this by creating a source dataset from your input data, applying a transformation to preprocess the data, then iterating over the dataset the specified number of elements at a time. This works because the iteration is streaming, so the data doesn't need to fit into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9916324-a2c4-4497-8495-5b65ef2a75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.0001,\n",
    "          num_epochs=1500, minibatch_size=32, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 10 epochs\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    costs, train_acc, test_acc -- lists for plotting/training history\n",
    "    \"\"\"\n",
    "    costs = []          # To keep track of the cost\n",
    "    train_acc = []      # To keep track of train accuracy history\n",
    "    test_acc = []       # To keep track of test accuracy history\n",
    "\n",
    "    # Initialize your parameters\n",
    "    parameters = initialize_parameters()\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    # The CategoricalAccuracy will track the accuracy for this multiclass problem\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    test_accuracy  = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    # Decide which reset method to use (compatible with TF \u22642.6 and \u22652.7)\n",
    "    if hasattr(train_accuracy, 'reset_state'):\n",
    "        reset_accuracy = lambda metric: metric.reset_state()\n",
    "    else:\n",
    "        reset_accuracy = lambda metric: metric.reset_states()\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
    "    test_dataset = tf.data.Dataset.zip((X_test, Y_test))\n",
    "\n",
    "    # We can get the number of elements of a dataset using the cardinality method\n",
    "    m = dataset.cardinality().numpy()\n",
    "\n",
    "    minibatches = dataset.batch(minibatch_size).prefetch(8)\n",
    "    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_total_loss = 0.\n",
    "\n",
    "        # Reset train accuracy at the beginning of each epoch\n",
    "        reset_accuracy(train_accuracy)\n",
    "\n",
    "        for (minibatch_X, minibatch_Y) in minibatches:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # 1. Forward propagation\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "                \n",
    "                # 2. Compute loss for this minibatch\n",
    "                minibatch_total_loss = compute_total_loss(Z3, tf.transpose(minibatch_Y))\n",
    "\n",
    "            # Accumulate accuracy (Y is one-hot, Z3 is logits \u2192 transpose Z3 to match)\n",
    "            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "\n",
    "            # Backpropagation\n",
    "            trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "            grads = tape.gradient(minibatch_total_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "            epoch_total_loss += minibatch_total_loss\n",
    "\n",
    "        # Average loss over all examples\n",
    "        epoch_total_loss /= m\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if print_cost and epoch % 10 == 0:\n",
    "            print(f\"Cost after epoch {epoch}: {epoch_total_loss:.4f}\")\n",
    "            print(\"Train accuracy:\", train_accuracy.result().numpy())\n",
    "\n",
    "            # Evaluate on test set (accumulate over test minibatches)\n",
    "            reset_accuracy(test_accuracy)\n",
    "            for (minibatch_X, minibatch_Y) in test_minibatches:\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "\n",
    "            print(\"Test accuracy:\", test_accuracy.result().numpy())\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # Save history\n",
    "            costs.append(epoch_total_loss)\n",
    "            train_acc.append(train_accuracy.result().numpy())\n",
    "            test_acc.append(test_accuracy.result().numpy())\n",
    "\n",
    "    return parameters, costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e655de9-e000-41ed-8380-001933a14021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Test the model\n",
    "parameters, costs, train_acc, test_acc = model(new_train, new_y_train, new_test, new_y_test, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c38cac-bb6f-4279-9cc4-985b137496c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9bd95-0965-4324-98f2-af79dce96476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train accuracy\n",
    "plt.plot(np.squeeze(train_acc))\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "# Plot the test accuracy\n",
    "plt.plot(np.squeeze(test_acc))\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a759070-a8d9-4cd9-bd0d-8211dbe9e59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}