{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71ff6a-d93b-4780-bcbc-df14cc9cdc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  #the resulting image is rendered as a static PNG directly below the code cell that produced it.\n",
    "%load_ext autoreload #it simply activates the functionality so that you can use the %autoreload command\n",
    "%autoreload 2 #Reload all modules (except those excluded) every time before executing the Python code typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead7d0c-8a76-4ca0-adb0-2bf51a702fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, Y = load_planar_dataset()\n",
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c353e04-e507-4467-a530-0c169ad68bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The shape of X is: ' + str(X.shape))\n",
    "print ('The shape of Y is: ' + str(Y.shape))\n",
    "print ('I have m = %d training examples!' % (X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1631c-ac0d-4cf9-8bce-2c4ef713bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check Logistic regression performance first. \n",
    "\n",
    "# Train the logistic regression classifier\n",
    "clf = sklearn.linear_model.LogisticRegressionCV();\n",
    "clf.fit(X.T, Y.T);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a8646-d544-4959-a983-fd7a7ada65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Boundary helper function\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "\n",
    "# Plot the decision boundary for logistic regression\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
    "plt.title(\"Logistic Regression\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77016e-c404-445f-bdb9-f2cceaea2a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "LR_predictions = clf.predict(X.T)\n",
    "print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) + '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffc9d7-3612-40f1-9b1d-418192f024ab",
   "metadata": {},
   "source": [
    "Interpretation: The dataset is not linearly separable, so logistic regression doesn't perform well. \n",
    "Hopefully a neural network will do better. Let's try this now!\n",
    "Next, you're going to train a Neural Network with a single hidden layer and see how that handles the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedab0c-23b5-41a4-a0d2-9720326ed3a4",
   "metadata": {},
   "source": [
    "The general methodology to build a Neural Network is to: \n",
    "1. Define the neural network structure ( # of input units, # of hidden units, etc). \n",
    "2. Initialize the model's parameters \n",
    "3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent)\n",
    "\n",
    "In practice, you'll often build helper functions to compute steps 1-3, \n",
    "then merge them into one function called nn_model().\n",
    "Once you've built nn_model() and learned the right parameters, you can make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d5564-af2f-4a73-bec3-50c3a7a1d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Define the neural network structure ( # of input units, # of hidden units, etc). \n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4          # size of hidden layer \n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    return (n_x, n_h, n_y)\n",
    "    \n",
    "def layer_sizes_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(5, 3)\n",
    "    Y_assess = np.random.randn(2, 3)\n",
    "    return X_assess, Y_assess\n",
    "\n",
    "t_X, t_Y = layer_sizes_test_case()\n",
    "(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b0463-8ed9-497b-9868-07d258c24fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Initialize the model's parameters \n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81e10d-24e1-4d95-8ada-80853ebb95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = 2, 4, 1\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df31cb4-86fc-45fb-85b3-6591d14b994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients \n",
    "#- Update parameters (gradient descent)\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "   \n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1) #tanh activation function used for first hidden layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = 1 / (1 + np.exp(-Z2))\n",
    "\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "\n",
    "    return A2, cache\n",
    "\n",
    "#define a test case\n",
    "def forward_propagation_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    b1 = np.random.randn(4,1)\n",
    "    b2 = np.array([[ -1.3]])\n",
    "\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': b1,\n",
    "     'b2': b2}\n",
    "\n",
    "    return X_assess, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11cb6a-12ef-4827-8b00-4cfd45d2b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_X, parameters = forward_propagation_test_case()\n",
    "A2, cache = forward_propagation(t_X, parameters)\n",
    "print(\"A2 = \" + str(A2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05026e5c-798e-45b8-8b40-0dbcceca38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Cost\n",
    "import numpy as np\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]  # number of examples\n",
    "\n",
    "    \"\"\" Optional code Avoid log(0) by clipping probabilities\n",
    "    epsilon = 1e-8\n",
    "    A2 = np.clip(A2, epsilon, 1 - epsilon)\n",
    "    \"\"\"\n",
    "\n",
    "    # Cross-entropy cost\n",
    "    logprobs = Y * np.log(A2) + (1 - Y) * np.log(1 - A2)\n",
    "    cost = -np.sum(logprobs) / m\n",
    "\n",
    "    cost = float(np.squeeze(cost))  # [[17]] \u2192 17\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc455f-a17c-4f12-b0b0-0524f513682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_test_case():\n",
    "    np.random.seed(1)\n",
    "    Y_assess = (np.random.randn(1, 3) > 0)\n",
    "\n",
    "    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))\n",
    "    \n",
    "    return a2, Y_assess\n",
    "\n",
    "A2, t_Y = compute_cost_test_case()\n",
    "cost = compute_cost(A2, t_Y)\n",
    "print(\"cost = \" + str(compute_cost(A2, t_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7527b-a0cb-4db3-8d57-5e04a1c8f4b5",
   "metadata": {},
   "source": [
    "Lets Perform Backpropagation\n",
    "Using the cache computed during forward propagation, we can now implement backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28502097-bc73-4d37-9509-ae516166bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = parameters[\"W1\"]   # shape (n_h, n_x)\n",
    "    W2 = parameters[\"W2\"]   # shape (n_y, n_h)\n",
    "    A1 = cache[\"A1\"]        # shape (n_h, m)\n",
    "    A2 = cache[\"A2\"]        # shape (n_y, m)\n",
    "\n",
    "    # Backward pass\n",
    "    dZ2 = A2 - Y                                 # shape (n_y, m)\n",
    "    dW2 = np.dot(dZ2, A1.T) / m                  # shape (n_y, n_h)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m  # shape (n_y, 1)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)                      # shape (n_h, m)\n",
    "    dZ1 = dA1 * (1 - np.power(A1, 2))            # derivative of tanh\n",
    "    dW1 = np.dot(dZ1, X.T) / m                   # shape (n_h, n_x)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m  # shape (n_h, 1)\n",
    "\n",
    "\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "\n",
    "def backward_propagation_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    Y_assess = (np.random.randn(1, 3) > 0)\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    "     'b2': np.array([[ 0.]])}\n",
    "\n",
    "    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n",
    "         [-0.05225116,  0.02725659, -0.02646251],\n",
    "         [-0.02009721,  0.0036869 ,  0.02883756],\n",
    "         [ 0.02152675, -0.01385234,  0.02599885]]),\n",
    "  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n",
    "  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n",
    "         [-0.05229879,  0.02726335, -0.02646869],\n",
    "         [-0.02009991,  0.00368692,  0.02884556],\n",
    "         [ 0.02153007, -0.01385322,  0.02600471]]),\n",
    "  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}\n",
    "    return parameters, cache, X_assess, Y_assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2972d2a-1e8b-477c-a83e-02d95ef3245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters, cache, t_X, t_Y = backward_propagation_test_case()\n",
    "\n",
    "grads = backward_propagation(parameters, cache, t_X, t_Y)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24942a8-4f2c-4645-a647-914bcb8011ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Paramaters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = copy.deepcopy(parameters[\"b1\"])\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = copy.deepcopy(parameters[\"b2\"])\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    # Update rule for each parameter\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d3806-aabc-4329-b7df-8848fab36d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_test_case():\n",
    "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "        [-0.02311792,  0.03137121],\n",
    "        [-0.0169217 , -0.01752545],\n",
    "        [ 0.00935436, -0.05018221]]),\n",
    " 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    " 'b1': np.array([[ -8.97523455e-07],\n",
    "        [  8.15562092e-06],\n",
    "        [  6.04810633e-07],\n",
    "        [ -2.54560700e-06]]),\n",
    " 'b2': np.array([[  9.14954378e-05]])}\n",
    "\n",
    "    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n",
    "        [ 0.00082222, -0.00700776],\n",
    "        [-0.00031831,  0.0028636 ],\n",
    "        [-0.00092857,  0.00809933]]),\n",
    " 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n",
    "          -2.55715317e-03]]),\n",
    " 'db1': np.array([[  1.05570087e-07],\n",
    "        [ -3.81814487e-06],\n",
    "        [ -1.90155145e-07],\n",
    "        [  5.46467802e-07]]),\n",
    " 'db2': np.array([[ -1.08923140e-05]])}\n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b75a88-e502-48af-8cf7-767c1bf3f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ab607-aa38-4770-b361-18552542c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrate all function and define the model\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15dc9b-83e1-40fb-875f-31361c459f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the Model\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    X -- input data of size (n_x, m)\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741546e5-7545-4e67-80b3-1979654e57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "        [-0.02311792,  0.03137121],\n",
    "        [-0.0169217 , -0.01752545],\n",
    "        [ 0.00935436, -0.05018221]]),\n",
    "     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    "     'b1': np.array([[ -8.97523455e-07],\n",
    "        [  8.15562092e-06],\n",
    "        [  6.04810633e-07],\n",
    "        [ -2.54560700e-06]]),\n",
    "     'b2': np.array([[  9.14954378e-05]])}\n",
    "    return parameters, X_assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2fb00-a902-4ced-8091-57f932cc08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, t_X = predict_test_case()\n",
    "\n",
    "predictions = predict(parameters, t_X)\n",
    "print(\"Predictions: \" + str(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a7b65-2b9b-4ae0-a39a-73f47c6cdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, time to test this model on the Original Planar Dataset from the beggining of this notebook\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cc8fc-f30d-428b-ab64-66e6d3cb3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cdf47-aede-49d9-993b-8f5124b264a1",
   "metadata": {},
   "source": [
    "Accuracy is really high compared to Logistic Regression. The model has learned the patterns of the flower's petals!\n",
    "Unlike logistic regression, neural networks are able to learn even highly non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd3796-92c0-44ba-a9b5-b6ed1c238ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning hidden layer size\n",
    "#This may take about 2 minutes to run\n",
    "\n",
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5]\n",
    "\n",
    "# you can try with different hidden layer sizes\n",
    "\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
    "\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7768b63-ac91-4330-83f5-4117ed6fbf6f",
   "metadata": {},
   "source": [
    "You can seee accuray peaked at 5 hidden layers. So, adding more layers does not aways increase accuracy.\n",
    "We can try other  dataset. you can rerun the whole notebook (minus the dataset part) for each of the following datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8a153-83c1-4165-a320-c6e11e1a8a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_extra_datasets():  \n",
    "    N = 200\n",
    "    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n",
    "    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n",
    "    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n",
    "    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n",
    "    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n",
    "    \n",
    "    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91922248-312f-4d2e-9d38-691d6141e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n",
    "datasets = {\"noisy_circles\": noisy_circles,\n",
    "            \"noisy_moons\": noisy_moons,\n",
    "            \"blobs\": blobs,\n",
    "            \"gaussian_quantiles\": gaussian_quantiles}\n",
    "\n",
    "# choose your dataset here\n",
    "dataset = \"noisy_moons\"\n",
    "\n",
    "X, Y = datasets[dataset]\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "\n",
    "# make blobs binary\n",
    "if dataset == \"blobs\":\n",
    "    Y = Y%2\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f2fcf-2560-40e5-a7da-d33d7d0eaf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}