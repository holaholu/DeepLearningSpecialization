{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953aa2e2-c9ea-4109-ab1c-0e8f3297817d",
   "metadata": {},
   "source": [
    "Goal :\n",
    "1.Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam\n",
    "2. Use random minibatches to accelerate convergence and improve optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8cb0ac-08a4-4207-8ab5-cda6ada93401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd063cbc-51a9-4ab3-a480-90064c44cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Start with Normal gradient Descent\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd41b197-5463-4f69-bec7-d07232b13219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 =\n",
      "[[ 1.63312395 -0.61217855 -0.5339999 ]\n",
      " [-1.06196243  0.85396039 -2.3105546 ]]\n",
      "b1 =\n",
      "[[ 1.73978682]\n",
      " [-0.77021546]]\n",
      "W2 =\n",
      "[[ 0.32587637 -0.24814147]\n",
      " [ 1.47146563 -2.05746183]\n",
      " [-0.32772076 -0.37713775]]\n",
      "b2 =\n",
      "[[ 1.13773698]\n",
      " [-1.09301954]\n",
      " [-0.16397615]]\n"
     ]
    }
   ],
   "source": [
    "#let test it\n",
    "np.random.seed(1)\n",
    "learning_rate = 0.01\n",
    "W1 = np.random.randn(2,3)\n",
    "b1 = np.random.randn(2,1)\n",
    "W2 = np.random.randn(3,2)\n",
    "b2 = np.random.randn(3,1)\n",
    "dW1 = np.random.randn(2,3)\n",
    "db1 = np.random.randn(2,1)\n",
    "dW2 = np.random.randn(3,2)\n",
    "db2 = np.random.randn(3,1)\n",
    "    \n",
    "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "\n",
    "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0403e-18d7-48bb-8252-8d9104b76ec3",
   "metadata": {},
   "source": [
    "# In Stochastic Gradient Descent, you use only 1 training example before updating the gradients.\n",
    "When the training set is large, SGD can be faster. \n",
    "But the parameters will \"oscillate\" toward the minimum rather than converge smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7fedf-09a1-4acb-a0c9-873c02b78fa1",
   "metadata": {},
   "source": [
    "Mini-batch gradient descent uses an intermediate number of examples for each step. \n",
    "With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples\n",
    "Shuffling and Partitioning are the two steps required to build mini-batches\n",
    "Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd73351-1bda-4ea8-b788-af2ff1c5a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as mine\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : m]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a9aa0d-254c-442c-a9b0-97d43319db03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (12288, 64)\n",
      "shape of the 2nd mini_batch_X: (12288, 64)\n",
      "shape of the 3rd mini_batch_X: (12288, 20)\n",
      "shape of the 1st mini_batch_Y: (1, 64)\n",
      "shape of the 2nd mini_batch_Y: (1, 64)\n",
      "shape of the 3rd mini_batch_Y: (1, 20)\n",
      "mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n"
     ]
    }
   ],
   "source": [
    "#lets test\n",
    "\n",
    "np.random.seed(1)\n",
    "mini_batch_size = 64\n",
    "X = np.random.randn(12288, 148)\n",
    "Y = np.random.randn(1, 148) < 0.5\n",
    "mini_batches = random_mini_batches(X, Y, mini_batch_size)\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape))\n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b35fe-5efb-4b66-8e8a-d76462ffa7fd",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd596ff-9b69-4e97-9af6-51d97bd633ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
