{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953aa2e2-c9ea-4109-ab1c-0e8f3297817d",
   "metadata": {},
   "source": [
    "Goal :\n",
    "1.Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam\n",
    "2. Use random minibatches to accelerate convergence and improve optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cb0ac-08a4-4207-8ab5-cda6ada93401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd063cbc-51a9-4ab3-a480-90064c44cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Start with Normal gradient Descent\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41b197-5463-4f69-bec7-d07232b13219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let test it\n",
    "np.random.seed(1)\n",
    "learning_rate = 0.01\n",
    "W1 = np.random.randn(2,3)\n",
    "b1 = np.random.randn(2,1)\n",
    "W2 = np.random.randn(3,2)\n",
    "b2 = np.random.randn(3,1)\n",
    "dW1 = np.random.randn(2,3)\n",
    "db1 = np.random.randn(2,1)\n",
    "dW2 = np.random.randn(3,2)\n",
    "db2 = np.random.randn(3,1)\n",
    "    \n",
    "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "\n",
    "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\\n\" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0403e-18d7-48bb-8252-8d9104b76ec3",
   "metadata": {},
   "source": [
    "# In Stochastic Gradient Descent, you use only 1 training example before updating the gradients.\n",
    "When the training set is large, SGD can be faster. \n",
    "But the parameters will \"oscillate\" toward the minimum rather than converge smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7fedf-09a1-4acb-a0c9-873c02b78fa1",
   "metadata": {},
   "source": [
    "Mini-batch gradient descent uses an intermediate number of examples for each step. \n",
    "With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples\n",
    "Shuffling and Partitioning are the two steps required to build mini-batches\n",
    "Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd73351-1bda-4ea8-b788-af2ff1c5a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as mine\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : m]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9aa0d-254c-442c-a9b0-97d43319db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets test\n",
    "\n",
    "np.random.seed(1)\n",
    "mini_batch_size = 64\n",
    "X = np.random.randn(12288, 148)\n",
    "Y = np.random.randn(1, 148) < 0.5\n",
    "mini_batches = random_mini_batches(X, Y, mini_batch_size)\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape))\n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b35fe-5efb-4b66-8e8a-d76462ffa7fd",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd596ff-9b69-4e97-9af6-51d97bd633ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "\n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "\n",
    "    # Initialize velocity\n",
    "    for l in range(1, L + 1):\n",
    "\n",
    "        v[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        v[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4da9e8-6e52-49e3-9703-e1478fbaa9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets test\n",
    "np.random.seed(1)\n",
    "W1 = np.random.randn(3,2)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(3,3)\n",
    "b2 = np.random.randn(3,1)\n",
    "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "v = initialize_velocity(parameters)\n",
    "print(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040837b-4cd2-4140-9749-ce3b764577bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using Momentum\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    v -- python dictionary containing your updated velocities\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Momentum update for each parameter\n",
    "    for l in range(1, L + 1):\n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l)] = beta * v[\"dW\" + str(l)] + (1 - beta) * grads[\"dW\" + str(l)]\n",
    "        v[\"db\" + str(l)] = beta * v[\"db\" + str(l)] + (1 - beta) * grads[\"db\" + str(l)]\n",
    "        \n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v[\"db\" + str(l)]\n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3075dca-e698-4a1d-9a2b-ebefbb5a3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets test Momentum update\n",
    "np.random.seed(1)\n",
    "W1 = np.random.randn(2,3)\n",
    "b1 = np.random.randn(2,1)\n",
    "W2 = np.random.randn(3,2)\n",
    "b2 = np.random.randn(3,1)\n",
    "\n",
    "dW1 = np.random.randn(2,3)\n",
    "db1 = np.random.randn(2,1)\n",
    "dW2 = np.random.randn(3,2)\n",
    "db2 = np.random.randn(3,1)\n",
    "   \n",
    "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "v = {'dW1': np.array([[ 0.,  0.,  0.],\n",
    "                          [ 0.,  0.,  0.]]), \n",
    "         'dW2': np.array([[ 0.,  0.],\n",
    "                          [ 0.,  0.],\n",
    "                          [ 0.,  0.]]), \n",
    "         'db1': np.array([[ 0.],\n",
    "                          [ 0.]]), \n",
    "         'db2': np.array([[ 0.],\n",
    "                          [ 0.],\n",
    "                          [ 0.]])}\n",
    "\n",
    "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
    "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f5112-93d2-418f-94b1-3af678912e59",
   "metadata": {},
   "source": [
    "The larger the momentum  \ud835\udefd is, the smoother the update, because it takes the past gradients into account more.\n",
    "But if  \ud835\udefd is too big, it could also smooth out the updates too much.\n",
    "Common values for  \ud835\udefd range from 0.8 to 0.999. If you don't feel inclined to tune this,  \ud835\udefd=0.9 is often a reasonable default.\n",
    "Tuning the optimal  \ud835\udefd for your model might require trying several values to see what works best in terms of reducing the value of the cost function  \ud835\udc3d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e6d95-1de5-43cd-b11e-4abf5671272a",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb584f6-f5ed-49d7-a9da-95c2ac03b6ba",
   "metadata": {},
   "source": [
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum.\n",
    "\n",
    "How does Adam work?\n",
    "\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables  \ud835\udc63 (before bias correction) and  \ud835\udc63\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 (with bias correction).\n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables  \ud835\udc60 (before bias correction) and  \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 (with bias correction).\n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76199b04-dfce-41aa-a68f-1256917aaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "\n",
    "    Returns:\n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient. Initialized with zeros.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(1, L + 1):\n",
    "\n",
    "        v[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        v[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "        s[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        s[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f955c20-123c-4824-9700-9b8a4a9b6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets test the initialization\n",
    "np.random.seed(1)\n",
    "W1 = np.random.randn(2,3)\n",
    "b1 = np.random.randn(2,1)\n",
    "W2 = np.random.randn(3,2)\n",
    "b2 = np.random.randn(3,1)\n",
    "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "\n",
    "v, s = initialize_adam(parameters)\n",
    "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76752446-96ac-41a7-a684-9597c489217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using the Adam optimization algorithm.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameter:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, maintained as a python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, maintained as a python dictionary\n",
    "    t -- Adam variable, counts the number of steps taken for bias correction\n",
    "    learning_rate -- learning rate, scalar\n",
    "    beta1 -- exponential decay hyperparameter for the first moment estimates\n",
    "    beta2 -- exponential decay hyperparameter for the second moment estimates\n",
    "    epsilon -- hyperparameter to prevent division by zero in the Adam update, a small scalar\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    v -- updated Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- updated Adam variable, moving average of the squared gradient, python dictionary\n",
    "    v_corrected -- python dictionary containing bias-corrected first moment estimates\n",
    "    s_corrected -- python dictionary containing bias-corrected second moment estimates\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "\n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1 - beta1) * grads[\"dW\" + str(l)]\n",
    "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1 - beta1) * grads[\"db\" + str(l)]\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - np.power(beta1, t))\n",
    "        \n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1 - beta2) * np.power(grads[\"dW\" + str(l)], 2)\n",
    "        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1 - beta2) * np.power(grads[\"db\" + str(l)], 2)\n",
    "        \n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - np.power(beta2, t))\n",
    "        \n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon)\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)] / (np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon)\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b419a81-9621-48f7-addc-83dd5f612004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam_test_case():\n",
    "    np.random.seed(1)\n",
    "    v, s = ({'dW1': np.array([[ 0.,  0.,  0.], # (2, 3)\n",
    "                              [ 0.,  0.,  0.]]), \n",
    "             'dW2': np.array([[ 0.,  0.],      # (3, 2)\n",
    "                              [ 0.,  0.],\n",
    "                              [ 0.,  0.]]), \n",
    "             'db1': np.array([[ 0.],           # (2, 1)\n",
    "                              [ 0.]]), \n",
    "             'db2': np.array([[ 0.],          # (3, 1)\n",
    "                              [ 0.],\n",
    "                              [ 0.]])}, \n",
    "            {'dW1': np.array([[ 0.,  0.,  0.], # (2, 3)\n",
    "                              [ 0.,  0.,  0.]]), \n",
    "             'dW2': np.array([[ 0.,  0.],      # (3, 2)\n",
    "                              [ 0.,  0.],\n",
    "                              [ 0.,  0.]]), \n",
    "             'db1': np.array([[ 0.],           # (2, 1)\n",
    "                              [ 0.]]), \n",
    "             'db2': np.array([[ 0.],           # (3, 1)\n",
    "                              [ 0.],\n",
    "                              [ 0.]])})\n",
    "    W1 = np.random.randn(2,3)\n",
    "    b1 = np.random.randn(2,1)\n",
    "    W2 = np.random.randn(3,2)\n",
    "    b2 = np.random.randn(3,1)\n",
    "\n",
    "    dW1 = np.random.randn(2,3)\n",
    "    db1 = np.random.randn(2,1)\n",
    "    dW2 = np.random.randn(3,2)\n",
    "    db2 = np.random.randn(3,1)\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    t = 2\n",
    "    learning_rate = 0.02\n",
    "    beta1 = 0.8\n",
    "    beta2 = 0.888\n",
    "    epsilon = 1e-2\n",
    "    \n",
    "    return parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon\n",
    "\n",
    "parametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon = update_parameters_with_adam_test_case()\n",
    "\n",
    "parameters, v, s, vc, sc  = update_parameters_with_adam(parametersi, grads, vi, si, t, learning_rate, beta1, beta2, epsilon)\n",
    "print(f\"W1 = \\n{parameters['W1']}\")\n",
    "print(f\"W2 = \\n{parameters['W2']}\")\n",
    "print(f\"b1 = \\n{parameters['b1']}\")\n",
    "print(f\"b2 = \\n{parameters['b2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f952614-caca-4ae3-b6f5-b9a05e6c0d39",
   "metadata": {},
   "source": [
    "## Model with different Optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f36ff-79b3-4c92-80ca-2bce8a2ee15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    np.random.seed(3)\n",
    "    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n",
    "    # Visualize the data\n",
    "    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "    \n",
    "    return train_X, train_Y\n",
    "\n",
    "train_X, train_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9686d96-ca2c-4ebf-b94f-129c24a0f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets us first import some helper functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "    \n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert parameters['W' + str(l)].shape[0] == layer_dims[l], layer_dims[l-1]\n",
    "        assert parameters['W' + str(l)].shape[0] == layer_dims[l], 1\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def compute_cost(a3, Y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- post-activation, output of forward propagation\n",
    "    Y -- \"true\" labels vector, same shape as a3\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the cost function without dividing by number of training examples\n",
    "    \n",
    "    Note: \n",
    "    This is used with mini-batches, \n",
    "    so we'll first accumulate costs over an entire epoch \n",
    "    and then divide by the m training examples\n",
    "    \"\"\"\n",
    "    \n",
    "    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "    cost_total =  np.sum(logprobs)\n",
    "    \n",
    "    return cost_total\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = 1./m * (a3 - Y)\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print results\n",
    "\n",
    "    #print (\"predictions: \" + str(p[0,:]))\n",
    "    #print (\"true labels: \" + str(y[0,:]))\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    \n",
    "def predict_dec(parameters, X):\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (a3 > 0.5)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8c649-6c0a-4258-a029-115a765b5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a 3-layer model with different optimization methods\n",
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    optimizer -- the optimizer to be passed, gradient descent, momentum or adam\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates\n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates\n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "\n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66075f-0003-465a-90cf-a3c60cf41cfc",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da52b1-0969-4468-bca9-b52d71e64bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model with \n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf9423-497b-4ad6-bf51-bea50c388331",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca1364-cac8-4dfb-b68b-7c3b030dc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Momentum optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25564eaf-619c-44fc-8442-c13525c09fbc",
   "metadata": {},
   "source": [
    "## Mini-Batch with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdff99-cb33-420b-b6aa-a0d8db101928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Adam optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d22d5-2944-4806-94f4-bf9740ef5128",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligible.\n",
    "\n",
    "On the other hand, Adam clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster.\n",
    "\n",
    "Some advantages of Adam include:\n",
    "\n",
    "Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum)\n",
    "Usually works well even with little tuning of hyperparameters (except  \ud835\udefc)\n",
    "\n",
    "Adam Paer: Adam paper: https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f583d4-3d10-467b-b74a-bacb3156d9da",
   "metadata": {},
   "source": [
    "## Learning Rate Decay and Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4aec57-c9bf-40b2-9624-8fee3e795a00",
   "metadata": {},
   "source": [
    "if you were to slowly reduce your learning rate alpha over time, you could then take smaller, slower steps that bring you closer to the minimum.\n",
    "This is the idea behind learning rate decay.\n",
    "We introduce two new parameters, decay and decay_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3626472-ded4-468e-a29a-61a362c392fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True, decay=None, decay_rate=1):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates\n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates\n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    lr_rates = []\n",
    "    learning_rate0 = learning_rate   # the original learning rate\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        if decay:\n",
    "            learning_rate = decay(learning_rate0, i, decay_rate)\n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "            if decay:\n",
    "                print(\"learning rate after epoch %i: %f\"%(i, learning_rate))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709c8b2-325e-41a3-a592-0a04541663de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the new learning rate using exponential weight decay.\n",
    "\n",
    "def update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "\n",
    "    learning_rate = learning_rate0 / (1 + decay_rate * epoch_num)\n",
    "\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968e688-28a1-4019-a93c-f5e64a5bb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "print(\"Original learning rate: \", learning_rate)\n",
    "epoch_num = 2\n",
    "decay_rate = 1\n",
    "learning_rate_2 = update_lr(learning_rate, epoch_num, decay_rate)\n",
    "\n",
    "print(\"Updated learning rate: \", learning_rate_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa466a-4811-4e85-b3b1-59fbe1f86568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=update_lr)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b0137-21fa-4e75-bc2f-52d72e0a553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that if you set the decay to occur at every iteration, the learning rate goes to zero too quickly - even if you start with a higher learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f32d90-c5aa-4feb-9a79-bb243d530513",
   "metadata": {},
   "source": [
    "## Fixed Interval Scheduling\n",
    "\n",
    "learning rate scheduled such that it only changes when the epochNum is a multiple of the timeInterval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d98420-6cb5-438b-8295-b4e33546a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n",
    "    \"\"\"\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "\n",
    "    Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer.\n",
    "    decay_rate -- Decay rate. Scalar.\n",
    "    time_interval -- Number of epochs where you update the learning rate.\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar\n",
    "    \"\"\"\n",
    "\n",
    "    learning_rate = learning_rate0 / (1 + decay_rate * np.floor(epoch_num / time_interval))\n",
    "\n",
    "\n",
    "    return learning_rate\n",
    "    \n",
    "learning_rate = 0.5\n",
    "print(\"Original learning rate: \", learning_rate)\n",
    "\n",
    "epoch_num_1 = 10\n",
    "epoch_num_2 = 100\n",
    "decay_rate = 0.3\n",
    "time_interval = 100\n",
    "learning_rate_1 = schedule_lr_decay(learning_rate, epoch_num_1, decay_rate, time_interval)\n",
    "learning_rate_2 = schedule_lr_decay(learning_rate, epoch_num_2, decay_rate, time_interval)\n",
    "print(\"Updated learning rate after {} epochs: \".format(epoch_num_1), learning_rate_1)\n",
    "print(\"Updated learning rate after {} epochs: \".format(epoch_num_2), learning_rate_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1694a7d6-7622-4b21-a93c-66daffe90e63",
   "metadata": {},
   "source": [
    "## Using Learning Rate Decay for each Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5d29a-f8e3-4c85-a5aa-ee296db1630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b511121-5f8e-441f-accd-1fcc785acc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"momentum\", learning_rate = 0.1, num_epochs=5000, decay=schedule_lr_decay)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Gradient Descent with momentum optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61f125-afec-46f5-b25f-bfa1bd17e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 3-layer model\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\", learning_rate = 0.01, num_epochs=5000, decay=schedule_lr_decay)\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Adam optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb294c-3ac7-42c5-bddd-37fa0900b191",
   "metadata": {},
   "source": [
    "With Mini-batch GD or Mini-batch GD with Momentum, the accuracy is significantly lower than Adam, but when learning rate decay is added on top, either can achieve performance at a speed and accuracy score that's similar to Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58f8fd-9d77-4d48-92b0-26581deea8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}