{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99058aa-6079-4e20-94fa-c75df74d2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecfb80-a1cc-4310-9dc3-f7bd4d392f8d",
   "metadata": {},
   "source": [
    "You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud\n",
    "--whenever someone makes a payment, you want to see if the payment might be fraudulent, \n",
    "such as if the user's account has been taken over by a hacker.\n",
    "\n",
    "You already know that backpropagation is quite challenging to implement, and sometimes has bugs. \n",
    "Because this is a mission-critical application, your company's CEO wants to be really certain that your implementation of backpropagation \n",
    "is correct. Your CEO says, \"Give me proof that your backpropagation is actually working!\" To give this reassurance, you are going to use\n",
    "\"gradient checking.\"\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c202e5-ea34-40eb-96d1-02c094aef6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first , lets define some helper functions\n",
    "def sigmoid(x):\n",
    "  \n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    " \n",
    "    s = np.maximum(0, x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        keys = keys + [key] * new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = theta[: 20].reshape((5, 4))\n",
    "    parameters[\"b1\"] = theta[20: 25].reshape((5, 1))\n",
    "    parameters[\"W2\"] = theta[25: 40].reshape((3, 5))\n",
    "    parameters[\"b2\"] = theta[40: 43].reshape((3, 1))\n",
    "    parameters[\"W3\"] = theta[43: 46].reshape((1, 3))\n",
    "    parameters[\"b3\"] = theta[46: 47].reshape((1, 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053ccc3-8179-47a4-9470-0a38f455f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n",
    "\n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "\n",
    "    Returns:\n",
    "    J -- the value of function J, computed using the formula J(theta) = theta * x\n",
    "    \"\"\"\n",
    "\n",
    "    J = theta * x\n",
    "    return J\n",
    "\n",
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6516a-7198-48b2-906e-d66111fd6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Computes the derivative of J with respect to theta (see Figure 1).\n",
    "\n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "\n",
    "    Returns:\n",
    "    dtheta -- the gradient of the cost with respect to theta\n",
    "\n",
    "    J(\u03b8) = \u03b8 \u00d7 x\n",
    "     The derivative of \u03b8 with respect to itself is 1\n",
    "     x is treated as a constant (the input)\n",
    "     So: dJ/d\u03b8 = x \u00d7 1 = x\n",
    "    \"\"\"\n",
    "    dtheta = x\n",
    "    return dtheta\n",
    "\n",
    "x, theta = 3, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2b7b4-de6c-4850-a372-acc6a22fff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- a float input\n",
    "    theta -- our parameter, a float as well\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient. Float output\n",
    "    \"\"\"\n",
    " \n",
    "    # Compute gradapprox using right side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n",
    "    # Step 1\n",
    "    theta_plus  = theta + epsilon\n",
    "    # Step 2\n",
    "    theta_minus = theta - epsilon\n",
    "    # Step 3\n",
    "    J_plus      = forward_propagation(x, theta_plus)\n",
    "    # Step 4\n",
    "    J_minus     = forward_propagation(x, theta_minus)\n",
    "    # Step 5\n",
    "    gradapprox  = (J_plus - J_minus) / (2 * epsilon)\n",
    "    \n",
    "    # Check if gradapprox is close enough to the output of backward_propagation()\n",
    "    grad = backward_propagation(x, theta)\n",
    "    \n",
    "    # Step 1'\n",
    "    numerator   = np.linalg.norm(grad - gradapprox)\n",
    "    # Step 2'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    # Step 3'\n",
    "    difference  = numerator / denominator\n",
    "    \n",
    "    \n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e878264-117d-42fe-a694-1264e38761c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta = 3, 4\n",
    "difference = gradient_check(x, theta, print_msg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673fe32-efaa-48b5-af02-3ea00efcb3b4",
   "metadata": {},
   "source": [
    "# N-Dimensional Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f19062-9cb6-4a31-9328-09bb64551278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "\n",
    "    Arguments:\n",
    "    X -- training set for m examples\n",
    "    Y -- labels for m examples\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (5, 4)\n",
    "                    b1 -- bias vector of shape (5, 1)\n",
    "                    W2 -- weight matrix of shape (3, 5)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "\n",
    "    Returns:\n",
    "    cost -- the cost function (logistic cost for m examples)\n",
    "    cache -- a tuple with the intermediate values (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve parameters\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Cost\n",
    "    log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1. / m * np.sum(log_probs)\n",
    "\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "\n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e828a-6c47-4563-b25d-0608c6d5c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    cache -- cache output from forward_propagation_n()\n",
    "\n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a2ec2-9222-441f-945b-2da82dd033ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters\n",
    "    X -- input datapoint, of shape (input size, number of examples)\n",
    "    Y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "\n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Set-up variables\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "\n",
    "\n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = None\n",
    "        # \"_\" is used because the function you have outputs two parameters but we only care about the first one\n",
    "        # Compute J_plus[i]\n",
    "        # Step 1\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        # Step 2\n",
    "        theta_plus[i][0] = theta_plus[i][0] + epsilon\n",
    "        parameters_plus = vector_to_dictionary(theta_plus) \n",
    "        # Step 3\n",
    "        J_plus[i], _ = forward_propagation_n( X, Y, parameters_plus,)\n",
    "\n",
    "        # Compute J_minus[i]\n",
    "        # Step 1\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        # Step 2\n",
    "        theta_minus[i][0] = theta_minus[i][0] - epsilon\n",
    "        parameters_minus = vector_to_dictionary(theta_minus)\n",
    "        # Step 3\n",
    "        J_minus[i], _ = forward_propagation_n( X, Y, parameters_minus,)\n",
    "\n",
    "        # Compute gradapprox[i]\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "\n",
    "       \n",
    "\n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    # Step 1'\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "\n",
    "    # Step 2'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "\n",
    "\n",
    "    # Step 3'\n",
    "    difference = numerator / denominator\n",
    "\n",
    "\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109623cb-189f-49dc-bb66-229da4dea56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets test it\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4,3)\n",
    "y = np.array([1, 1, 0])\n",
    "W1 = np.random.randn(5,4) \n",
    "b1 = np.random.randn(5,1) \n",
    "W2 = np.random.randn(3,5) \n",
    "b2 = np.random.randn(3,1) \n",
    "W3 = np.random.randn(1,3) \n",
    "b3 = np.random.randn(1,1) \n",
    "parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3 }\n",
    "\n",
    "cost, cache = forward_propagation_n(x, y, parameters)\n",
    "gradients = backward_propagation_n(x, y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, x, y, 1e-7, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964e97d-2ff9-489c-b098-17cd9c8fbfe3",
   "metadata": {},
   "source": [
    "#Well, looks like there were error in the backward_propagation_n \n",
    "\n",
    "The code contains two deliberate \"bugs\" :\n",
    "\n",
    "Line dW2: It is being multiplied by 2, which is incorrect.\n",
    "Line db1: It is dividing 4. / m, but it should be 1. / m.\n",
    "    \n",
    "Here is the corrected code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab7767-362d-452b-bba6-d12a8cdb02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    cache -- cache output from forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    # FIX 1: Removed \"* 2\" at the end\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    # FIX 2: Changed \"4. / m\" to \"1. / m\"\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df9575-f12e-4477-bff7-8bb1a999ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Lets test again\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4,3)\n",
    "y = np.array([1, 1, 0])\n",
    "W1 = np.random.randn(5,4) \n",
    "b1 = np.random.randn(5,1) \n",
    "W2 = np.random.randn(3,5) \n",
    "b2 = np.random.randn(3,1) \n",
    "W3 = np.random.randn(1,3) \n",
    "b3 = np.random.randn(1,1) \n",
    "parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3 }\n",
    "\n",
    "cost, cache = forward_propagation_n(x, y, parameters)\n",
    "gradients = backward_propagation_n(x, y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, x, y, 1e-7, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a140a49-640a-4763-912d-58c4c826393c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}