{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc547a5-a587-4d8f-b7d9-593c8fc1743b",
   "metadata": {},
   "source": [
    "# Transformer Network Application: Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048c9bf-c6e2-49b2-a10e-9f23c47f5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "tf.get_logger().setLevel('ERROR') #This line turns off almost all non-error messages printed by TensorFlow during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb2f9a-1f5c-4a32-b1bc-9a1c23739c34",
   "metadata": {},
   "source": [
    "## 1 - Named-Entity Recogniton to Process Resumes\n",
    "\n",
    "When faced with a large amount of unstructured text data, named-entity recognition (NER) can help you detect and classify important information in your dataset. For instance, in the running example \"Jane vists Africa in September\", NER would help you detect \"Jane\", \"Africa\", and \"September\" as named-entities and classify them as person, location, and time. \n",
    "\n",
    "* You will use a variation of the Transformer model we built in TransformerNetwork.ipynb to process a large dataset of resumes.\n",
    "* You will find and classify relavent information such as the companies the applicant worked at, skills, type of degree, etc.\n",
    "\n",
    "### 1.1 - Dataset Cleaning\n",
    "\n",
    "In this notebook,  you will optimize a Transformer model on a dataset of resumes. Take a look at how the data you will be working with are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b24c3-b855-415f-838a-06281e3f40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './model_data'\n",
    "df_data = pd.read_json(f\"{BASE_DIR}/ner.json\", lines=True)\n",
    "df_data = df_data.drop(['extras'], axis=1)\n",
    "df_data['content'] = df_data['content'].str.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9b633-85e8-443e-8225-2f1995dc2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b588fdf-bf1e-4de0-9117-fb95d0834ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.iloc[0]['annotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8c2f9-bef9-40f4-8c1b-2fe5c8150e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeIntervals(intervals):\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                if lower[2] is higher[2]:\n",
    "                    upper_bound = max(lower[1], higher[1])\n",
    "                    merged[-1] = (lower[0], upper_bound, lower[2])\n",
    "                else:\n",
    "                    if lower[1] > higher[1]:\n",
    "                        merged[-1] = lower\n",
    "                    else:\n",
    "                        merged[-1] = (lower[0], higher[1], higher[2])\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f8d38-1ee1-47ab-bdc5-a0a1222ec42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(df):\n",
    "    \n",
    "    entities = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        entity = []\n",
    "    \n",
    "        for annot in df['annotation'][i]:\n",
    "            try:\n",
    "                ent = annot['label'][0]\n",
    "                start = annot['points'][0]['start']\n",
    "                end = annot['points'][0]['end'] + 1\n",
    "                entity.append((start, end, ent))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        entity = mergeIntervals(entity)\n",
    "        entities.append(entity)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94568ff-81ea-480c-ac7c-204916bc6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['entities'] = get_entities(df_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f655cb-ede4-403b-bca9-73f5ed389f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "            entities = []\n",
    "            data_annotations = data['annotation']\n",
    "            if data_annotations is not None:\n",
    "                for annotation in data_annotations:\n",
    "                    #only a single point in text annotation.\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    # handle both list of labels or a single label.\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "                        \n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start = point_start + lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end = point_end - rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1 , label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a0282-e9e9-4956-b16f-fc48277a8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy(f\"{BASE_DIR}/ner.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05efed7b-3129-4ac8-b7ff-8f9ce4bea628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def clean_dataset(data):\n",
    "    cleanedDF = pd.DataFrame(columns=[\"setences_cleaned\"])\n",
    "    sum1 = 0\n",
    "    for i in tqdm(range(len(data))):\n",
    "        start = 0\n",
    "        emptyList = [\"Empty\"] * len(data[i][0].split())\n",
    "        numberOfWords = 0\n",
    "        lenOfString = len(data[i][0])\n",
    "        strData = data[i][0]\n",
    "        strDictData = data[i][1]\n",
    "        lastIndexOfSpace = strData.rfind(' ')\n",
    "        for i in range(lenOfString):\n",
    "            if (strData[i]==\" \" and strData[i+1]!=\" \"):\n",
    "                for k,v in strDictData.items():\n",
    "                    for j in range(len(v)):\n",
    "                        entList = v[len(v)-j-1]\n",
    "                        if (start>=int(entList[0]) and i<=int(entList[1])):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                start = i + 1  \n",
    "                numberOfWords += 1\n",
    "            if (i == lastIndexOfSpace):\n",
    "                for j in range(len(v)):\n",
    "                        entList = v[len(v)-j-1]\n",
    "                        if (lastIndexOfSpace>=int(entList[0]) and lenOfString<=int(entList[1])):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            numberOfWords += 1\n",
    "        cleanedDF = pd.concat([cleanedDF, pd.Series([emptyList], index=cleanedDF.columns).to_frame().T],ignore_index=True)\n",
    "        sum1 = sum1 + numberOfWords\n",
    "    return cleanedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931e617-8dc4-43b0-96f1-948736b86e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDF = clean_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f195db-b248-436d-8ff7-3364f5317b57",
   "metadata": {},
   "source": [
    "Take a look at your cleaned dataset and the categories the named-entities are matched to, or 'tags'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1318302-89f9-4c00-addc-e34930c0ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371e589-f31f-4c3b-9874-25d6ad6e1c95",
   "metadata": {},
   "source": [
    "1.2 - Padding and Generating Tags\u00b6\n",
    "Now, it is time to generate a list of unique tags you will match the named-entities to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339752e7-e469-48b8-a817-958fb3ba8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(cleanedDF['setences_cleaned'].explode().unique())#pd.unique(cleanedDF['setences_cleaned'])#set(tag for doc in cleanedDF['setences_cleaned'].values.tolist() for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b50c95-a861-40f3-96cb-617f2ba0391a",
   "metadata": {},
   "source": [
    "unique_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515b073-83e9-485e-8a22-bf29870dd341",
   "metadata": {},
   "source": [
    "Next, you will create an array of tags from your cleaned dataset. Oftentimes, your input sequence can exceeds the maximum length of a sequence your network can process, so it needs to be cut off to that desired maximum length. And when the input sequence is shorter than the desired length, you need to append zeroes onto its end using this [Keras padding API](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec84d4c-92dc-45d6-a5ec-a6a835666615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb2447-5b8b-4ffb-88c8-0d53f0fc0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "labels = cleanedDF['setences_cleaned'].values.tolist()\n",
    "\n",
    "tags = pad_sequences([[tag2id.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2id[\"Empty\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f027fb4-28f0-4b84-b9cb-c2beb5c1bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2366ff7-f3c7-4806-b4a5-4ac6a634b96d",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Tokenize and Align Labels with \ud83e\udd17 Library\n",
    "\n",
    "Before feeding the texts to a Transformer model, you will need to tokenize your input using a [\ud83e\udd17 Transformer tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html). It is crucial that the tokenizer you use must match the Transformer model type you are using! In this exercise, you will use the \ud83e\udd17 [DistilBERT fast tokenizer](https://huggingface.co/transformers/model_doc/distilbert.html), which standardizes the length of your sequence to 512 and pads with zeros. Notice this matches the maximum length you used when creating tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588190da-6f3a-4b0f-bdd5-8dee767ee151",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f931a8-e3b9-402f-82f5-5a2872b81c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.48.0   # or 4.50+, check pypi for your date\n",
    "\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(f'{BASE_DIR}/tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66906d-916e-4efb-bdd2-7deca134f2c1",
   "metadata": {},
   "source": [
    "Transformer models are often trained by tokenizers that split words into subwords. For instance, the word 'Africa' might get split into multiple subtokens. This can create some misalignment between the list of tags for the dataset and the list of labels generated by the tokenizer, since the tokenizer can split one word into several, or add special tokens. Before processing, it is important that you align the lists of tags and the list of labels generated by the selected tokenizer with a `tokenize_and_align_labels()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504f639-039e-40e5-b7f2-b187825bf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(tokenizer, examples, tags):\n",
    "    tokenized_inputs = tokenizer(examples, truncation=True, is_split_into_words=False, padding='max_length', max_length=512)\n",
    "    labels = []\n",
    "    for i, label in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2f43d-c039-41c6-bee1-1ed1bd8fdf67",
   "metadata": {},
   "source": [
    "Now that you have tokenized inputs, you can create train and test datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977eeda7-4724-461b-ab5f-7ab3c72c6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenize_and_align_labels(tokenizer, df_data['content'].values.tolist(), tags)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    test['input_ids'],\n",
    "    test['labels']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ceebdf-727b-4a1c-bfbe-9e819f2e43a9",
   "metadata": {},
   "source": [
    "### 1.4 - Optimization\n",
    "\n",
    "Fantastic! Now you can finally feed your data into into a pretrained \ud83e\udd17 model. You will optimize a DistilBERT model, which matches the tokenizer you used to preprocess your data. Try playing around with the different hyperparamters to improve your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cff0f-45d1-41fc-9e59-bfe51a78618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tf-keras\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    f'{BASE_DIR}/model/',\n",
    "    num_labels=len(unique_tags)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad77475-6538-4863-98ba-d6fbb2012e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f5a2a-5da5-4975-94d1-48b2e622254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",               # string \u2192 transformers/tf_keras handles it internally\n",
    "    loss=model.hf_compute_loss,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(train_dataset.batch(4),\n",
    "          epochs=10, \n",
    "          batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d50258-1513-4ea0-a886-3ab7e993e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Manisha Bharti. 3.5 years of professional IT experience in Banking and Finance domain\"\n",
    "inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, is_split_into_words=False, padding=\"max_length\", max_length=512 )\n",
    "input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301c9af-c2eb-43d6-96c0-0df9bb352f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(inputs).logits\n",
    "prediction = np.argmax(output, axis=2)\n",
    "print( prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4578b60-8b84-4e1c-9a5e-b25ecff0d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fe8e7-b397-4019-9dc9-df15d2e09b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4efb6-0306-4156-a302-8992500cb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [[id2tag.get(true_index, \"Empty\") for true_index in test['labels'][i]] for i in range(len(test['labels']))]\n",
    "np.array(true_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee83b8-7ca9-4f7b-b7be-a4df1fbce32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8df82-44cb-41e6-988e-36485ffe845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(output['logits'].reshape(220, -1, 12), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75686940-c801-468c-8b76-0913ca1e1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dc616-5abd-40be-8f9b-e8aa4f59ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "p = plt.hist(np.array(true_labels).flatten())\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa233e-71c9-4691-8fe7-c404d6759e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(np.array(true_labels).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2ddb8-081b-4270-a81e-3bf09b68068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [[id2tag.get(index, \"Empty\") for index in predictions[i]] for i in range(len(predictions))]\n",
    "p = plt.hist(np.array(pred_labels).flatten())\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5b1e3-0815-47b5-8bbd-1142c533a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval\n",
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201c3cd-7c10-4ee7-ac86-c2374d75927c",
   "metadata": {},
   "source": [
    "\n",
    "#### Here's what you should remember\n",
    "\n",
    "- Named-entity recognition (NER) detects and classifies named-entities, and can help process resumes, customer reviews, browsing histories, etc. \n",
    "- You must preprocess text data with the corresponding tokenizer to the pretrained model before feeding your input into your Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ae6d7-21f2-47d2-a3b8-82b1e37eded2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLspe (Python 3.13)",
   "language": "python",
   "name": "dlspe_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}